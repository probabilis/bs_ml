"""
Author: Maximilian Gschaider
MN: 12030366
"""
from bo_utilization import plot_approximation, plot_acquisition, plot_convergence
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize, Bounds
from scipy.stats import norm
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import ConstantKernel, Matern
import sys
sys.path.append('../')
from repo_utils import repo_path, fontsize, fontsize_title
####################################################

prefix = "test_"

def expected_improvement(X, X_sample, gpr, kappa = 0.01, threshold = 1E-100):
    '''
    Calculates the Exptected Improvement (= EI) at the domain points X based on existing samples X_sample
    and Y_sample using a Gaussian process as surrogate model.

    Params :
        X : nested array
            Grid points at which EI will be computed (n x d)
        X_sample : nested array
            Sample locations (2 x d)
        Y_sample : nested array
            Sample values (n x 1)
        gpr : method
            GaussianProcessRegressor fitting method
        kappa : float
            Exploitation-Exploration trade-off parameter
        threshold : flaot
            Threshold number for sigma = 0 evaluation
    Returns : nested array
        Expected improvements (EI) of domain points X
    '''
    #fitting the whole domain to a Gaussian process
    mu, sigma = gpr.predict(X, return_std=True)
    #fitting the sample domain to a Gaussian process
    mu_sample = gpr.predict(X_sample)

    #calculating the expected improvement (EI) 
    with np.errstate(divide='warn'):
        improvement = mu - np.max(mu_sample) - kappa
        Z = improvement / sigma
        #using the cumulative distribution function (CDF) and probability density function (PDF) of a normal distribution
        EI = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)
        #EI at sigma = 0 is zero / implemented via a threshold because of uncertainty
        EI[sigma < threshold] = 0.0

    return EI

def propose_next_sampling_point(acquisition, X_sample, gpr, bounds, random_points = 200, min_value = 0.1, min_x = None):
    '''
    This function propopes the next sampling point in the search space by optimizing the acquisition function

    Params :
        acquisition : method
            acquisition function with params af(X, X_sample,Y_sample, gpr)
        X_sample : nested array
            sample locations (n x d).
        Y_sample : nested array
            sample values (n x 1).
        gpr : method
            GaussianProcessRegressor fitting method
        bounds : list
            bounds of objective function
        random points : int
            amount of random points generated by uniform distribution to sample to minimize the objective function

    Returns : nested array
        The spatial location of the acquisition function maximum
    '''
    x_dim = X_sample.shape[1]

    #objective function for optimization / minimization the negative acquisition function
    def acquisition_opt(X):
        return - acquisition(X.reshape(-1, x_dim), X_sample, gpr)

    #Through iteratively sample random points (in total random_points) of a uniform distribituion the optimum gets aproximated 
    #For the optimization techniqe the quasi-Newton method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) will be used because of effiency.
    #It uses the first derivatives only L-BFGS-B uses the L-BFGS-B algorithm for bound constrained minimization.
    #Ref.: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize

    bounds_seq = Bounds(lb = bounds[0], ub = bounds[1], keep_feasible=False)

    for x0 in np.random.uniform(bounds[0], bounds[1], size=(random_points, x_dim)):
        minimum = minimize(acquisition_opt, x0 = x0, bounds = bounds_seq, method='L-BFGS-B')

        if minimum.fun < min_value:
            min_value = minimum.fun
            min_x = minimum.x

    return min_x.reshape(-1, 1)

####################################################

bounds = [-2.0, 3.0] #creating nested X array within bounds
X = np.arange(bounds[0], bounds[1], 0.01).reshape(-1, 1) #sample points
X_sample = np.array([[-1.9], [2.2]]) #additive noise ratio
noise = 0.3

#test-objective function
def objective_function(X, noise = noise):
    '''
    objective function for BO
    Params:
        X : nested array
            input array
        noise : float
            noise parameter
    Returns : nested array
        function values Y
    '''
    return 10 + - np.sin(5*X) + np.exp(-(X - 1)**2) - X**2 + 1.5*X + noise * np.random.randn(*X.shape)

Y_sample = objective_function(X_sample) #noisy objective function
Y = objective_function(X,0) #noise-free objective function values at X 

def plot_objective_function(plot_save = True):
    #plot objective function with noise level which should be optimized
    plt.plot(X, Y, color = 'salmon', linestyle = "--", lw=2, label='Noise-free objective Z(x)')
    plt.plot(X, objective_function(X), color = 'cornflowerblue', marker = "x", lw=1, alpha=0.3, label='Noisy samples Z(x) + $\mathcal{N}(\cdot)$')
    plt.plot(X_sample, Y_sample, 'kx', mew=3, label='Initial samples')
    plt.xlabel("$x$", fontsize = fontsize)
    plt.ylabel("$Z(x)$", fontsize = fontsize)
    plt.title("Objective function $Z(x) = -sin(5x) + e^{2(x-1)} - x^2$ + 1,5$x + 10 + \mathcal{N}(\cdot)$", fontsize = fontsize_title)
    plt.legend()
    fig = plt.gcf()
    fig.set_size_inches(8,4)
    plt.tight_layout()
    if plot_save == True:
        plt.savefig(repo_path + "/figures/" + prefix + "bayesian_optimization_from_scratch_objective.png", dpi=300)
    plt.show()

plot_objective_function()

#Gaussian process with Matern kernel as surrogate model
#Matern kernel : https://en.wikipedia.org/wiki/Mat%C3%A9rn_covariance_function
m52 = ConstantKernel(1.0) * Matern(length_scale = 1.0, nu = 3)
gpr = GaussianProcessRegressor(kernel = m52, alpha = noise**2)

#Number of iterations which have to be made
number_iterations = 8

def plot_bo_iterations(X_sample, Y_sample, plot_save = True):

    plt.figure(figsize=(12, number_iterations * 2))
    plt.subplots_adjust(hspace = 0.2)

    for i in range(number_iterations):
        #Step 1 : updating the Gaussian process with existing samples
        gpr.fit(X_sample, Y_sample)

        #Step 2 : obtaining the next Sampling Point from the acquisition function (expected_improvement) = (EI)
        X_next = propose_next_sampling_point(expected_improvement, X_sample, gpr, bounds)

        #Step 3 : obtaining the next noisy sample from the objective function
        Y_next = objective_function(X_next, noise)

        #Plot the sample points, surrogate function, noise-free objective and next sampling location
        plt.subplot(number_iterations, 2, 2 * i + 1)
        plot_approximation(gpr, X, Y, X_sample, Y_sample, X_next, show_legend=i==0)
        plt.title(f'Iteration {i+1}', fontsize = 12)
        #calculating the current expected improvement (EI) for plotting
        ei_ = expected_improvement(X, X_sample, gpr)#[:][-1]
        #plotting acquisition function
        plt.subplot(number_iterations, 2, 2 * i + 2)
        plot_acquisition(X, ei_, X_next, show_legend=i==0)

        #Step 5 : Add sample to previous samples
        X_sample = np.vstack((X_sample, X_next))
        Y_sample = np.vstack((Y_sample, Y_next))

    #fig.suptitle(f"Bayesian optimization of scalar objetive function $F(x)$ over {n_iter} iterations with Gaussian Prior")
    plt.tight_layout()
    if plot_save == True:
        plt.savefig(repo_path + "/figures/" + prefix + "bayesian_optimization_from_scratch_iterations.png", dpi=300)
    plt.show()
    return X_sample, Y_sample

X_sample, Y_sample = plot_bo_iterations(X_sample, Y_sample)

plot_convergence(X_sample, Y_sample)
plt.savefig(repo_path + "/figures/" + prefix + "bayesian_optimization_from_scratch_convergence_plot.png", dpi=300)
plt.show()
