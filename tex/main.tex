\documentclass[12pt, a4paper]{article}
\usepackage{amsmath}
%\usepackage{german}
%\usepackage[ngerman]{babel}
\usepackage{graphicx}
\usepackage[skip=5pt,font=scriptsize,justification=justified,singlelinecheck=false]{caption}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{siunitx}
\usepackage{textcomp}
\usepackage{accents}
\usepackage{enumitem}
\usepackage{color}
\usepackage{comment}
%\usepackage[version=3]{mhchem}
\usepackage{url}
\usepackage{pdfpages}
\usepackage{upgreek}
\usepackage{scrlayer-scrpage}
\pagestyle{scrheadings}
\usepackage{caption}
\usepackage{amsfonts}
\usepackage{physics}
\usepackage{hyperref}
\usepackage{pdfpages}
\usepackage{biblatex}
\usepackage{amssymb}
\usepackage{titlepic}
\usepackage{forest}
\usepackage{geometry}
\usepackage{dsfont}
\geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm%,margin = 1cm
 }
\usepackage[linesnumbered,ruled]{algorithm2e}
%\usepackage{algorithm}
%\usepackage{algorithmic}
%\usepackage{algcompatible}
\usepackage{algpseudocode}
\addbibresource{literatur.bib}
%\usepackage{subfigure}
\DeclareUnicodeCharacter{2212}{-}
\usepackage[labelfont={bf,sf},font={small},%
labelsep=space]{caption}
\usepackage{doi}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Ein Makro für Bezug auf Abbildungen
\newcommand{\fref}[1]{\figurename\ \ref{#1}}
% Ein Makro für Bezug auf Seiten
\newcommand{\pref}[1]{\pagename\ \pageref{#1}}
% Ein Makro fuer Bezug auf eine Section
\newcommand{\sref}[1]{section\ \ref{#1}}
% Ein Makro fuer Bezug auf Zeile in Codelistings
\newcommand{\lref}[1]{Line\ \ref{#1}}
% Ein Makro fuer Bezug auf ein Listing
\newcommand{\Lref}[1]{Listing\ \ref{#1}}
% Ein Makro fuer Bezug auf Tabellen
\newcommand{\tref}[1]{\tablename\ \ref{#1}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\ihead{Maximilian Gschaider}
    \chead{GBMDT/BO}
    \ohead{24.01.2024}
\cfoot*{\pagemark}%

\newpage
\thispagestyle{empty}

\newpage
\clearpage

\title{
Gradient Boosting Machine \\
based on Decision Tree Regressor \\
\large tuned by Bayesian Optimization \\
\vspace{1cm}\\
\textbf{Technical University Graz}\\
Institute of Theoretical and Computational Physics}
\author{
\\\\Maximilian Gschaider\\ \href{mailto:gschaider@student.tugraz.at}{gschaider@student.tugraz.at} \\
\vspace{1cm}\\
{\small Univ.$-$Prof. Dr. Wolfgang von der Linden \\
\small Advisor: Dr. Sascha Ranftl} }
\titlepic{\includegraphics[width=220]{figures/TU_Graz.png}}
\date{24.01.2024}
\maketitle
%\thispagestyle{empty}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section*{Abstract}
Gradient Boosting is a supervised machine learning technique which is used in regression and classification tasks. Because of its prediction speed, accuracy and interpretability especially with large and complex data structures it attracted a lot of attention in the last decades. It's a special kind of ensemble method where multiple learning models are combined to obtain better predictive performance than could be obtained from any of the constituent learning models alone. Through converting weak learners to strong ones one can benefit of the boosting structure. In comparison to Bootstrap Aggregating (Bagging), Stacking or Voting the Gradient Boosting framework is built in a stage-wise fashion. Usually in regression tasks the weak learners are decision trees, because of its natural intelligibility and simplicity. Initially \textit{Schapire} and \textit{Freund} developed the first statistical meta-algorithm framework called \textit{Adaboost}, an adaptive boosting algorithm that won the \textit{Gödel Prize}. The idea of gradient boosting originated in the observation by \textit{Leo Breiman} that boosting can be interpreted as an optimization algorithm on a suitable cost function. The first explicit regression gradient boosting algorithms were subsequently developed by \textit{Jerome H. Friedman}. \\
In the following thesis the Gradient Boosting algorithm will be illuminated from depth especially with Regression Trees as weak learners. The Bayesian Optimization method will be introduced for tuning the hyperparameters of the model. For practical purposes the algorithms will be designed from scratch and finally the frameworks will be used to make inference (predictions) in a real-world data problem. Additionally the Gradient Boosting Machine will be compared to a Neural Network in the same prediction problem to highlight strengths and weaknesses.
\clearpage
\thispagestyle{empty}
\tableofcontents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\clearpage
\setcounter{page}{1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newcommand\h{$h(\vec{x};\vec{a}_m)$}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
In the dynamic field of machine learning and predictive modeling, the search for accurate and efficient algorithms is a constant challenge. The quest for meaningful insights from data and accurate inference has driven the development of increasingly sophisticated techniques. \\
Beside Neural Networks and Support Vector Machines, which are often used in supervised machine learning tasks, Gradient Boosting Machines (GBM) have emerged as a powerful and versatile algorithm. \\
At the \textit{heart} of the Gradient Boosting Machine lies the fusion of two fundamental concepts: \textit{ensemble learning} and \textit{decision trees}. Through combining multiple ``weak'' models to one ``strong'' model one benefits of the additive base learner characteristic. This is in comparison to Neural Networks a total different modeling structure where the loss of multiple features do not get computed in parallel but moreover sequentially. \\
Decision trees, being intuitive and interpretable, have been a powerful tool in the field of classification and regression tasks for decades. They partition data into subsets based on feature attributes and recursively make decisions, resulting in a tree-like structure that can provide valuable insights into the decision-making process. However, standalone implemented decision trees often suffer from overfitting and suboptimal predictive performance. Because of this challenges ensemble learning techniques have gained a lot of attention.
Gradient Boosting, in particular, has demonstrated exceptional capability in enhancing the predictive accuracy of decision trees. By iteratively building an ensemble of decision trees and adjusting their weights based on the errors made by the preceding trees, Gradient Boosting Machines can significantly improve predictive performance while mitigating overfitting concerns. \\
This thesis embarks on a comprehensive exploration of Gradient Boosting Machines built upon decision trees as a foundational framework. \\
The overarching objective of this study is to explore the synergy between Gradient Boosting with Decision Trees and Bayesian Optimization, merging the strengths of ensemble learning with informative hyperparameter tuning. In pursuit of this goal, I endeavor to accomplish the following key objectives:\\
\vspace{0.5cm} \\
\textbf{In-Depth algorithmic understanding:}  \\
Investigate the foundational principles of Gradient Boosting and Decision Tree Regressor, establishing a robust theoretical fundament. \\
\vspace{0.3cm} \\
\textbf{Machine Learning framework built from scratch:} \\
Construct a Gradient Boosting Machine grounded in Decision Tree Regressors. \\
\vspace{0.3cm} \\
\textbf{Model optimization built from scratch:} \\
Implement Bayesian Optimization as a versatile and resilient optimization technique, adept at efficiently navigating and optimizing the hyperparameter landscape. \\
\vspace{0.3cm} \\
\textbf{Practical applications:} \\
Throughout this thesis, a practical test on a case (test)-study with the implemented algorithms from scratch is presented and in addition a real-world data problem from the financial market is explored. In addition a short comparison with a Multi Layer Perception Neural Network (MLP-NN) will be carried out.
\newpage
The corresponding Github repository \cite{Gschaider} can be viewed at the following link:
\href{https://github.com/probabilis/bs_ml}{Github Repository for BSc ML Project}.
The project folder is structured as follows:
\vspace{0.5cm} \\
\begin{forest}
  for tree={
    font=\ttfamily,
    grow'=0,
    child anchor=west,
    parent anchor=south,
    anchor=west,
    calign=first,
    edge path={
      \noexpand\path [draw, \forestoption{edge}]
      (!u.south west) +(7.5pt,0) |- node[fill,inner sep=1.25pt] {} (.child anchor)\forestoption{edge label};
    },
    before typesetting nodes={
      if n=1
        {insert before={[,phantom]}}
        {}
    },
    fit=band,
    before computing xy={l=15pt},
  }
[bs\_ml
  [analysis]
  [bayesian\_opt]
  [figures]
  [from\_scratch]
  [models]
  [neuralnetwork]
  [preprocessing]
  [research]
  [rounds]
  [test]
  [teststudy]
  [tex]
  [...
    [main.py]
    [repo\_utils.py]
    [requirements.txt]
  ]
]
\end{forest}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Methodology}
The basic methodology and theoretical framework of GBM's algorithm, as originally derived by Friedman et al. \cite{Friedman2001} and Breiman et al. \cite{Breiman1984}. This overview is considered as an introduction and therefore the strict mathematical proofs of the derivations and their properties are not covered fully. In addition the fundamentals and notations in this chapter closely follow the presentation in the stated references.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Function estimation}
In the function estimation or ``predictive learning'' problem, one has a system of ``input'' or ``explanatory'' variables $\vec{x} = \{x_1,...,x_n\}$ and a depending ``output'' or ``response'' variable $y$. The goal is to reconstruct the unknown functional dependence $\vec{x} \xrightarrow{F} y$ with the estimate $\tilde{F}(\vec{x})$, such that some specified loss function $L(y,F)$ is minimized.
\begin{center}
    $\tilde{F}(\vec{x}) = \underset{F(\vec{x})}{\arg\min} L(y,F(\vec{x}))$
\end{center}
Expanding this notion, by using this training sample $\{y,\vec{x}_n\}$ with the known $(y,\vec{x})$- values one can obtain an estimate or approximation $\tilde{F}(\vec{x})$ of the function $F(\vec{x})$ mapping $\vec{x}$ to $y$ which minimizes the expected value $\mathbb{E}$ (first moment in probability theory) of some specified loss function $L(y,F(\vec{x}))$ over the joint distribution of all $(y,\vec{x})$ -values:
\begin{equation}
    \tilde{F} = \underset{F}{\arg\min} \mathbb{E}_{y,\vec{x}} L(y,F(\vec{x})) = \underset{F}{\arg\min} \underbrace{\mathbb{E}_{\vec{x}} [\overbrace{\mathbb{E}_y (L(y,F(\vec{x})))}^\text{expected $y$ loss}|\vec{x}]}_\text{expectation over the whole dataset}
\end{equation}
Typically used loss functions (estimators) for regression problems $L(y,F)$ include squared-error $(y - F)^2$ and absolute-error $|y - F|$ for $y \in \mathbb{R}^1$.
Often the function to be determined $F(\vec{x})$ is restricted to be a member of a parameterized class of functions $F(\vec{x},\vec{P})$, where $\vec{P} = \{P_1,P_2,..\}$ is a discrete set of parameters whose joint values identify individual class members. Because of ``brave'' modeling behavior one can use only ``additive'' expansions of the form:
\begin{equation}
    F(\vec{x};\{\beta_m, \vec{a}_m\}_1^M) = \sum_{m=1}^{M} \beta_m h(\vec{x};\vec{a}_m)
    \label{eq: param_approx_function}
\end{equation}
The function $h(\vec{x};\vec{a}_m)$ is called a generic function (can handle different data types) and is usually a simple parameterized function of the input variables $\vec{x}$ by specific parameters $\vec{a} = \{a_1,a_2,...\}$. Depending on the joint values $\vec{a}_m$ chosen for these parameters the individual terms will differ. Expansions like (\ref{eq: param_approx_function}) are the basis of many function approximation methods such as neural networks or support vector machines. When working with a continuous domain of the variables, the generic functions $h(\vec{x};\vec{a}_m)$ are often used as small regression trees, such as those produced by \textit{CART}$^{TM}$ algorithm \cite{Breiman1984}. For such regression trees the parameters $\vec{a}_m$ are the splitting variables, split locations and the terminal node means of the individual trees, but this will be discussed in section [\ref{sec: decision_trees}].
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Numerical optimization in function space}
\label{sec: parameter_opt}
At numerical optimization methods, a distinction is often made between parametric and non-parametric approaches. While both aim to draw inferences from data, they differ mainly in their underlying principles.
At parametric methods typically it is assumed that the data follows a known probability distribution where  non-parametric approaches do not make any assumptions about the form of the mapping function. Another perspective is the amount and structure of data which is available for the model. Generally speaking one can say that parametric models are simpler and require less data, but as already stated they assume a certain data structure.\\
In the following derivation it is considered that the initial function $F(\vec{x})$ evaluation at each point $\vec{x}$ acts as a ``parameter'' for the model, as Friedman et al. stated \cite{Friedman2001}. With this assumption one can consequently define the minimization of the loss function between the unknown functional dependence $F(\vec{x})$ and known output values $y$ through the expectation value:
\begin{center}
    $\Phi(F) = \mathbb{E}_{y,\vec{x}} L(y,F(\vec{x})) = \mathbb{E}_{\vec{x}}[\mathbb{E}_y(L(y,F(\vec{x})))|\vec{x}]$
\end{center}
Whereby the expectation $\mathbb{E}_{y,\vec{x}}$ can once again be split up by chaining up the expectation $\mathbb{E}_{\vec{x}}$ with respect to the expectation $\mathbb{E}_y$ over the loss function given by the input vector $\vec{x}$. This can be shortened by evaluating directly at each individual $\vec{x}$ with respect to $F(\vec{x})$
\begin{center}
    $\Phi(F(\vec{x})) = \mathbb{E}_y [L(y,F(\vec{x}))|\vec{x}]$
\end{center}
Theoretically there are an infinite number of such parameters in function space, but in data sets are only a finite number $\{F(\vec{x}_i)\}_1^N$ given. Following the numerical optimization paradigm we take solutions of the additive form:
\begin{center}
    $\tilde{F}(\vec{x}) = \sum_{m=0}^{M} f_m(\vec{x})$
\end{center}
Whereby $f_0(\vec{x})$ is an initial guess and $\{f_m(\vec{x})\}_1^M$ are incremental functions (``steps'' or ``boosts'') defined by the optimization method. There are several different numerical optimization methods which are often used in function optimization tasks. Mostly one will choose an iterative method, like ``Gradient descent'' resp. ``Steepest descennt''. \\
\subsection{Steepest descent}
Steepest descent (also known as gradient descent) is an unconstrained optimization technique which uses a first-order (linear local error) iterative algorithm schematic. The main idea is to take repeated steps in the opposite direction of the gradient at the current point (direction of steepest descent). Gradient descent is based on the observation that if the function $F(\vec{x})$ is defined and differentiable in a given neighborhood of a point $\vec{p}$, then the function $F(\vec{x})$ decreases ``steepest'' in the direction of the negative gradient $- \nabla F(\vec{p})$. From there one can conclude that following expression
\begin{equation}
    \vec{p}_{n+1} = p_n - \gamma \nabla F(\vec{a}_n)
\end{equation}
can ultimately converge to a local minimum $F(\vec{a_n}) \geq F(\vec{a}_{n+1})$, under the constrain that $\gamma \in \mathbb{R}_{+}$ act as small step size. This monotonic sequence will approach the desired local minimum and if $F(\vec{x})$ is even a convex function it will approach the global minimum. By using this steepest-descent method for the $m$-th incremental function results to
\begin{equation}
    \label{eq: incr_function}
    f_m(\vec{x}) = - \rho_m \cdot g_m(\vec{x})
\end{equation}
and with the definition of the gradient:
\begin{center}
    $g_m(\vec{x}) = [\frac{\partial \Phi(F(\vec{x}))}{\partial F(\vec{x})}]_{F(\vec{x}) = F_{m-1}(\vec{x})} = [\frac{\partial \mathbb{E}_y [L(y,F(\vec{x})|\vec{x}]}{\partial F(\vec{x})}]_{F(\vec{x}) = F_{m-1}(\vec{x})}$
\end{center}
and the total accumulation over all incremental functions:
\begin{center}
    $F_{m-1}(\vec{x}) = \sum_{i=0}^{m-1} f_i(\vec{x})$
\end{center}
One must now assume that the following function suffices regularity and consequently differentiation and integration can be interchanged by integrating the expectation value $\mathbb{E}_y$ over the function values $F(\vec{x})$.
\begin{equation}
    g_m(\vec{x}) = \mathbb{E}_y \left[\frac{\partial L(y, F(\vec{x}))}{\partial F(\vec{x})} | \vec{x}\right]_{F(\vec{x}) = F_{m-1}(\vec{x})}
\end{equation}
The factor $\rho_m$ in (\ref{eq: incr_function}) is given by minimizing the line search
\begin{equation}
    \rho_m = \underset{\rho}{\arg\min} \mathbb{E}_{y,\vec{x}} L[y,F_{m-1}(\vec{x}) - \rho \vec{g}_m(\vec{x})]
\end{equation}
This represents the classical numerical optimization algorithm in function space.
\begin{comment}
One can now switch to the parameterized model $F(\vec{x};\vec{P})$ (fixed number of parameters with respect to the sample size) and redefine the optimization process.
\subsection{Parameterized function optimization}
By choosing a parameterized model $F(\vec{x};\vec{P})$ the function optimization problem becomes a parameter optimization problem, whereby a parameterized model is normally characterized by a fixed number of parameters with respect to the sample size.:
\begin{equation}
    \label{eq: argmin_P}
    \tilde{\vec{P}} = \underset{P}{\arg\min} \Phi(\vec{P})
\end{equation}
where
\begin{center}
    $\Phi(\vec{P}) = \mathbb{E}_{y,\vec{x}} L(y,F(\vec{x};\vec{P})) $
\end{center}
is the risk function for the parameter optimization and $\mathbb{E}_{y,\vec{x}}$ defines the expectation over all output values $y$ and input vectors $\vec{x}$. From this it is evident that the following structure can be derived, where $\tilde{F}$ generally denotes an estimate in this notation.
\begin{center}
    $\tilde{F}(\vec{x}) = F(\vec{x};\tilde{\vec{P}})$
\end{center}
For most function estimations $F(\vec{x};\tilde{\vec{P}})$ and loss functions $L$ suitable numerical optimizations methods must be applied to solve (\ref{eq: argmin_P}). The solution for the parameters can be expressed in the form
\begin{equation}
    \label{eq: lc_par}
    \tilde{\vec{P}} = \sum_{m=0}^{M} \vec{p}_m
\end{equation}
of a linear combination, where $\vec{p}_0$ is an initial guess and $\{\vec{p_m}\}_1^M$ are successive increments (``steps'' or ``boosts''), each based on the sequence of preceding steps. The rule for computing of the individual steps $\vec{p}_m$ is defined by the specifically used optimization method.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Steepest-descent}
Steepest-descent is an unconstrained optimization technique and often used as numerical minimization methods when dealing with a real valued function $f: \mathbb{R}^n \rightarrow \mathbb{R}$. The idea is to

Therefore the increments $\{\vec{p}\}_1^M$ (\ref{eq: lc_par}) are defined as follows. First the current gradient $\vec{g}_m$ is computed:
\begin{center}
    $\vec{g}_m = \{g_{jm}\} = \nabla \Phi(\vec{P}) = \{ [\frac{\partial \Phi(\vec{P})}{\partial P_j}]_{\vec{P}=\vec{P}_{m-1}} \}$ with $\vec{P}_{m-1} = \sum_{m=0}^{m-1} \vec{p}_i$
\end{center}
The incremental step in the direction of the steepest-descent is taken to be
\begin{equation}
    \label{eq: gradient}
    \vec{p}_m = - \rho_m \vec{g}_m \text{   with   } \rho_m = \underset{\rho}{\arg\min} \Phi(\vec{P}_{m-1} - \rho \vec{g}_m).
\end{equation}
with a stepsize which results from the minimization of the parameterized solution with the evaluated steepest descent. Subsequently the ``steepest-descent'' is defined by the negative gradient \text{-}$\vec{g}_m$ direction (\ref{eq: gradient}) and is called the ``line search'' along that direction.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Barriers with finite data and GB design}
When dealing with a finite data-set the non-parametric approach breaks down because the joint distribution of $(y,\vec{x})$ is estimated only by this discrete set of data samples $\{y_i,\vec{x}_i\}_1^N$, as Friedman originally stated \cite{Friedman2001}. This is because the expectation $\mathbb{E}_y[\cdot|\vec{x}]$ cannot be estimated accurately by its data value at each specific $\vec{x}_i$. Although it is not possible, one would also like to estimate $\tilde{F}(\vec{x})$ at $\vec{x}$ values other than the given training sample points. \textit{Therefore strength and stability for the system must be borrowed from nearby data points by imposing smoothness on the solution} \cite{Friedman2001}.
Friedman et al. supposed here an elegant way for providing a solution to this problem by assuming a parameterized form such as (\ref{eq: param_approx_function}) and performing a parameter optimization as discussed in section [\ref{sec: parameter_opt}] to minimize the corresponding data based estimate of the expected loss:
\begin{center}
    $\{\beta_m,\vec{a}_m\}_1^M = \underset{ \{\beta'_m,\vec{a}'_m \} }{\arg\min}
    \sum_{i=1}^N L
    \left(y_i,\sum_{m=1}^M\beta'_{m} h(\vec{x}_i;\vec{a}'_m)\right)$
\end{center}
Due the fact that the summation over all $m$ in the argument of the loss function will be computationally impractical a ``greedy-stagewise'' approach will be implemented. Therefore for $m$ = 1,2,...,$M$ the following simplification will be used for optimizing the loss function
\begin{equation}
    \label{eq: greedy_stagewise}
        \{\beta_m,\vec{a}_m\} = \underset{ \{\beta_m,\vec{a}_m \} }{\arg\min}
    \sum_{i=1}^N L
    (y_i,F_{m-1}(\vec{x_i}) + \beta_m h(\vec{x}_i;\vec{a}_m))
\end{equation}
for evaluating the stagewise function
\begin{equation}
    F_m(\vec{x}) = F_{m-1}(\vec{x}) + \beta_m h(\vec{x};\vec{a}_m).
\end{equation}
It's definitely visible that by taking the last evaluated approximation instead of all generic functions in the evaluation of the loss function one gets a more efficient minimization calculation.
It must be noted that this \textit{stagewise} strategy is different from a stepwise approach that reconfigure previously entered terms when new ones are added. This means that only the new set of parameters $\{\beta_m,\vec{a}_m\}$ will be optimized and the previous ones are frozen.
The function $h(\vec{x};\vec{a}_m)$ is called a ``weak'' or ``base learner''. \\
In addition it can be assumed that for a particular loss $L(y,F)$ and base learner $h(\vec{x};\vec{a}_m)$ the solution to (\ref{eq: greedy_stagewise}) is difficult to obtain, like Friedman mentioned in his official paper.
\\
\textit{Given any approximator $F_{m-1}(\vec{x})$ the function $\beta_m h(\vec{x};\vec{a})$ can be viewed the best greedy step towards the data based estimate of $\tilde{F}(\vec{x})$, under the constraint that the step ``direction'' $h(\vec{x};\vec{a}_m)$ be a member of the parameterized class of functions $h(\vec{x};\vec{a})$}, as Friedman et al. stated \cite{Friedman2001}. It can therefore be considered as the \textit{steepest descent} under that constraint. By the general construction, the data-based analogue of the unconstrained negative gradient,
\begin{center}
    $- g_m(\vec{x}_i) = - [\frac{\partial L(y_i, F(\vec{x}_i))}{\partial F(\vec{x}_i)}]_{F(\vec{x}) = F_{m-1}(\vec{x})}$
\end{center}
gives the best steepest-descent step direction $-\vec{g}_m = \{-g_m(\vec{x}_i)\}_1^N$ in the $N$-dimensional data space at $F_{m-1}$.
But this gradient is defined only at the data points $\{\vec{x}_i\}_1^N$ and cannot be generalized to other $\vec{x}$-values.
To bypass this limitation one can generalize the solution by choosing that member of the parameterized class $h(\vec{x};\vec{a}_m)$ that produces $\vec{h}_{m} = \{h(\vec{x}_i ;\vec{a})\}_1^N$ most parallel to $-\vec{g}_m \in \mathbb{R}^N$. \textit{This is the $h(\vec{x};\vec{a})$ most highly correlated with $- g_m(\vec{x})$ over the data distribution}, as Friedman et al. \cite{Friedman2001} proposed. This workaround for gaining information about the gradient can be obtained from the minimization of the solution:
\begin{equation}
    \vec{a}_m = \underset{ \vec{a}}{\arg\min}
    \sum_{i=1}^N [- g_{m}(\vec{x_i}) - \beta h(\vec{x}_i;\vec{a})]^2
\end{equation}
\textit{This constrained negative gradient $h(\vec{x};\vec{a})$ is used in place of the unconstrained one $-\vec{g}_m(\vec{x})$ in the steepest-descent strategy}. Afterwards the line search is evaluated with respect to the minimization criteria
\begin{equation}
        \rho_m = \underset{\rho_m}{\arg\min} \sum_{i=1}^N L[y_i,F_{m-1}(\vec{x}_i) + \rho_m h(\vec{x}_i;\vec{a}_m)]
\end{equation}
and then the approximation updated:
\begin{equation}
    F_{m} = F_{m-1}(\vec{x}) + \rho_m h(\vec{x};\vec{a}_m)
\end{equation}
\textit{Basically, instead of obtaining the solution under a smoothness constraint, the constraint is applied to the unconstrained (rough) solution by fitting $h(\vec{x};\vec{a})$ to the ``pseudo-responses'' $\{\tilde{y}_i = - g_m(\vec{x}_i)\}_{i=1}^N$} (squared error of the real gradient to the pseudo-gradient). \\
Where $\tilde{y}$ denotes the estimator which is equivalent to the negative gradient $- g_m(\vec{x}_i)$ of the corresponding loss function.
\textit{This permits the replacement of the difficult function minimization problem by least-squares function minimization, followed by only a single parameter optimization based on the original criterion}, as stated by Friedman et al. \cite{Friedman2001}.\\
So for any base learner function $h(\vec{x};\vec{a})$ for which an implementable least-squares algorithm exists for solving, one can use this approach to minimize any differentiable loss or objective function $L(y,F)$.
As shown, this is achieved with a forward stage-wise additive modeling method by using steepest-descent as numerical optimization technique.
\begin{algorithm}
\caption{Gradient Boosting \cite{Friedman2001}}
\label{alg: gb_algo}
    $F_0(\vec{x}) = \underset{\rho}{\arg\min} \sum_{i=1}^N L(y_i,\rho)$ \\
    \For{$m = 1$ to $M$}
    {$\tilde{y}_i = [\frac{\partial L(y_i, F(\vec{x}_i))}{\partial F(\vec{x}_i)}]_{F(\vec{x}) = F_{m-1}(\vec{x})} \qquad i = 1,..,N$ \\
    $\vec{a}_m = \underset{ \vec{a}_m}{\arg\min}
    \sum_{i=1}^N [\tilde{y}_i - \beta h(\vec{x}_i;\vec{a}_m)]^2$ \\
    $\rho_m = \underset{\rho_m}{\arg\min} \sum_{i=1}^N L[y_i,F_{m-1}(\vec{x}_i) + \rho_m h(\vec{x}_i;\vec{a}_m)]$ \\
    $F_{m} = F_{m-1}(\vec{x}) + \rho_m h(\vec{x};\vec{a}_m)$
    }
\end{algorithm}
\\
Basically the gradient in the associated function space is approximated by a \textit{parameterized function}. The parametrization then allows to generalize towards unseen data inputs. This can also be constructed directly, the noteworthy think here is that it has been derived starting from function space which alows to depart into more general settings. \\
In general any fitting criterion can be used that estimates the conditional expectation (given $\vec{x}$) to evaluate the (smooth) negative gradient at line 3 of the Gradient Boosting generic algorithm [Alg. \ref{alg: gb_algo}]. Because of the universal and efficient computational properties of least-squares implementations one will select it when dealing with regression tasks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Loss function}
In mathematical optimization a loss function is a function that maps an event or values of one or more variables onto a real number intuitively representing some ``cost'' associated with the event. Through optimization one can seek to minimize the loss function. In this paper we will focus only on quadratic loss-functions, because of the usability in regression tasks.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Least-squares regression}
Defining the loss function as $L(y,F) = \frac{1}{2} (y - F)^2$ results in quadratic residuals with symmetric properties. Therefor the ``pseudo-responses'' in line 3 of the GB-algorithm [\ref{alg: gb_ls_algo}] result in the difference $\tilde{y}_i = y_i - F_{m-1}(\vec{x}_i)$. The minimization in the following line 4 results in fitting the current residuals and the line search produces the result $\rho_m = \beta_m$, where the parameter minimization of $\beta_m$ is already included when calculating $\vec{a}_m$. This results in the classical gradient boosting algorithm [\ref{alg: gb_ls_algo}] with least-squares regression which follows the usual stagewise approach by iteratively fitting the current residuals.
\begin{algorithm}
\caption{Gradient Boosting with least-squares regression \cite{Friedman2001}}
\label{alg: gb_ls_algo}
    $F_0(\vec{x}) = \bar{y}$ \\
    \For{$m = 1$ to $M$}
    {
    $\tilde{y}_i = y_i - F_{m-1}(\vec{x}_i) \qquad i = 1,..,N$ \\
    $\{\rho_m, \vec{a}_m \} = \underset{ \vec{a}, \rho }{\arg\min}
    \sum_{i=1}^N [\tilde{y}_i - \rho h(\vec{x}_i;\vec{a})]^2$ \\
    $F_{m} = F_{m-1}(\vec{x}) + \rho_m h(\vec{x};\vec{a}_m)$
    }
\end{algorithm}
\\
The resulting additive model combines the information of individual base learners $h(\vec{x}_i;\vec{a})$ into one ensemble model.
\newpage
\section{Base learners for the additive modeling structure}
\subsection{Decision trees}
A decision tree is a flowchart like structure where on each node a binary decisions will be made. The tree is built by splitting the initial data set into subsets. \\
Depending on the inherent properties of the input features $\vec{x}_i$ of the dataset one can distinguish of different metrics which are classically used. Generally these metrics measure the ``homogeneity'' of the target variable within the subsets. Depending on the discrete or continuous properties of the data-set one can use different metrics like positive correctness, gini-impurity, information gain, variance reduction or goodness of fit. \\
This splitting process is repeated on each derived subset in a recursive manner called recursive partitioning. Usually the recursion is stopped when the splitting no longer adds value to the prediction or some threshold is met. This specific process of top-down induction of decision trees is an example of a ``greedy'' algorithm. The term \textbf{classification and regression tree (CART)} analysis is an umbrella term used to refer either regression or classification predictions \cite{Steinberg2009}.The Gradient Boosting framework creates multiple decision trees based on the given hyperparameters because of its ensemble properties. This is done by incrementally building an ensemble by training each new instance to emphasize the training instances previously ``mis-modeled''.
\begin{comment}
More generally, the concept of regression tree can be extended to any kind of object equipped with pairwise dissimilarities such as categorical sequences.
A decision tree is a simple representation for classifying examples. When dealing with features with a binary output it is a straightforward process. Through recursively building the tree from the root node (also called external / terminal node) as a top down approach one gets the f

Assume that all the input features $\vec{x} = \{x_1,...,x_n\}$ have finite discrete domains and there's a single target feature $C_y$ called the ``classification''.

Each element of the domain of the classification is called a ``class''. A decision tree is a tree in which each internal / terminal (non-leaf) node is labeled with an input feature. The links coming from a node labeled with an input feature are labeled with each of the possible values of the target feature or the link leads to a subordinate decision node on a different input feature. Each leaf of the tree is labeled with a class or a probability distribution over the classes, signifying that the dataset has been classified by the tree into either a specific class, or into a particular probability distribution.
The tree is built by splitting the source set, constituting the root node of the tree, into subsets, which iteratively constitute the successor children.\\
The splitting is based on a set of splitting rules based on the inherent nature of features (regression or classification task). This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the subset at a node has all the same values of the target variable $y$ or when splitting no longer adds value to the predictions. This process of ``top-down induction of decision trees'' is an example of a greedy algorithm and it is by far the most common strategy for learning decision trees from data.
\end{comment}
\subsubsection{Regression trees}
\label{sec: decision_trees}
When dealing with continuous variables ($\mathcal{X, Y} \subseteq \mathbb{R}$) in the dataset it's called a regression tree. The most common metric for measuring the information gain in the splitting process is called ``variance reduction'', which initially got introduced in CART \cite{Breiman1984}. \\
A decision tree $T$ is presentable via a function $f_T : \mathcal{X} \rightarrow \mathcal{Y}$ which assigns each input a prediction of the output. The classical CART algorithm \cite{Breiman1984} finds self-reliant branches (nodes) and splitting rules for optimal allocation. As stated one can use different metrics for calculating the information gain. We'll focus on the ``variance reduction'' based on mean square error (MSE) deviation for the loss function function,
\begin{center}
    $L(T,\mathcal{D}) = \frac{1}{m} \sum_{i=1}^m (f_T(x^{(i)}) - y_i)^2$
\end{center}
which has then to be minimized. Whereby $\mathcal{D} = \{(x^{(i)},y_i)\}_{i=1}^n$ is the training's data-set with input variables $x^{(i)} = (x_1^{i} ,...,x_n^{i}) \in \mathcal{X}$ and output variables $y \in \mathcal{Y}$. An input variable over multiple training samples is generally denoted as feature vector $\vec{x}$ and represents a specific property of the dataset. \\
For each leaf $l$ a subset $R_l$ will be assigned, that for each $L$ leafs associated disjoint sets form a partition of the initial total region. One can now search an estimated value for all $x^{(i)} \in R_l$ that best approximates the true value $\{y_i | x^{(i)} \in R_l\}$.
The estimator
\begin{center}
    $f_T(x^{(i)} \in R_l) = \mathbb{E}[y_i | x^{(i)} \in R_l] = \hat{c}_l$
\end{center}
with expectation to $y$ provides a solution therefor. Because the distribution is per se not known it will be approximated in most computational cases by the classical ``average''.
In addition the computational effort for calculation all possible trees is not efficient feasible one can use once again a ``greedy'' algorithm.
\textit{One starts with a tree which consists of only one node and then successively find locally optimal branching.}, as Steinberg stated in his official paper \cite{Steinberg2009}. \\
At each node the individual feature $j$ will be calculated which splits the entry in two disjoint regions of the parent nodes the best. For non-ordinal features (similarly to non-categorical) one can use the following metric (\ref{eq: dt_metric}):
\begin{equation}
    \label{eq: dt_metric}
    \underset{ j,s }{\min} \left(
    \underset{ \hat{c}_1 }{\min}
    \sum_{x^{(i)} \in R_1(j,s)} (y_i - \hat{c}_1)^2 +
    \underset{ \hat{c}_2 }{\min}
    \sum_{x^{(i)} \in R_2(j,s)} (y_i - \hat{c}_2)^2
    \right)
\end{equation}
Whereby $\hat{c}_l = \mathbb{E}[y_i | x^{(i)} \in R_l(j,s)]_{l = 1,2}$ in each case minimizes the two sums which gives an appropriate estimator for $\hat{y}^{\ast}$ in each region [\ref{fig: rt_regions}].
With the following definitions of the ``left'' $R_1(j,s) = \{x^{(i)} | x_j^{i} \leq S\}$ and ``right'' $R_2(j,s) = \{x^{(i)} | x_j^{i} > S\}$ sub-region for all $x^{i}$ in the initial partition.
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.4\textwidth,trim={0 0 0 0},clip]{figures/bs_ml_figure_0.png}
    \caption[Regression Tree node structure and splitting regions]{Regression Tree node structure and splitting regions}
    \label{fig: rt_regions}
\end{figure}
Based on the single node each step two new nodes will be added which in turn are branched further until a termination condition (e.g.: the maximum path length from root to leaves / $d_{max}$ = maximal tree depth) is met. \\
If there's only one input feature one can visualize the recursive branching process of the decision tree regressor in the following way (for simplicity the function and splitting are roughly sketched). Given the input set $x\in \mathbb{R}$ with a random sampled function $f(x)$:
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.9\textwidth,trim={0 0 0 0},clip]{figures/bs_ml_figure_1.png}
    \caption[Regression Tree splitting schematic illustration]{Regression Tree splitting schematic illustration with $d_{max} = 2$ / left: splitting structure based on metric (\ref{eq: dt_metric}) / right: sampled function $f(x)$ with estimators $\hat{c}_l$ }
    \label{fig: rt_splitting_schematic}
\end{figure}
\\
In the beginning [Fig. \ref{fig: rt_splitting_schematic}] the input feature $x$ will be optimally split (depth 1) based on all sample points in the entire region $R_l$ which build the function $y(x)$. This is the case for $x > 2$ based on the metric for $\hat{c}_l$ which provides on each side a solution, namely the ``mean'' of the sample-points in the respective region as visible.
If we split one level further (depth 2) this process is repeated whereby the initial domain $R_l$ is reduced. The optimal branching is calculated again through the estimators $\hat{c}_l$ in the nested regions and at the end of each recursion (the leaf) the mean is assigned.\\
When having multiple features $\vec{x}_i$ as input, the splitting points become higher dimensional objects depending one the features room dimension. In figure [\ref{fig: rt_spliting_dim}] this is shown for a pseudo-splitting schematic in the case of three-dimensional ($d$ = 3) feature-room. The splitting procedure creates multiple disjoint regions based on the amount of features and tree depth.
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.5\textwidth,trim={0 0 0 0},clip]{figures/bs_ml_figure_2.png}
    \caption[Regression Tree higher dimensional splitting structure]{Regression Tree higher dimensional splitting structure in the case of $d$ = 3 features / visible are 1st and 2nd split and a partition $R^{\ast}(j,s)$}
    \label{fig: rt_spliting_dim}
\end{figure}
\\
In addition one can use this relative rank (i.e. depth) of a feature to assess the relative importance of it with respect to the predictability of the target variable. Features used at the top of the tree contribute to the final prediction decision of a larger fraction of the input samples. The expected fraction of the samples they contribute to can thus be used as an estimate of the relative importance of the features (= ``feature importance''). \\
In the following there's a simplified tree-growing algorithm from sketch given [Alg. \ref{alg: tree_growing_algo}]:
\begin{algorithm}
\caption{Simplified tree-growing algorithm sketch \cite{Steinberg2009}}\label{alg: tree_growing_algo}
    Assign all training data to the root node \\
    Define the root node as a terminal node \\
    \underline{SPLIT:} \\
    $NewSplits \gets 0$ \\
    \For{every terminal node in the tree}
    {
        \eIf{the terminal node sample size is too small or all instances in the node belong to the same target class}
        {\textbf{end}}
        {Find the attribute that best separates the node into two child nodes using an appropriate splitting metric (\ref{eq: dt_metric})}
        $NewSplits \gets NewSplits + 1$
    }
\end{algorithm}
\\
So decision trees are very ``natural'' constructions because of the splitting or branching structure, in particular when explanatory variables are categorical (and even better, when they're binary) but also possible when they are real numbers as stated. Another huge advantage is that the created models are invariant under transformations in the predictor space, as Breiman et al. \cite{Breiman1984} stated.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Regression boosted trees}
\label{sec: regr_boosted_trees}
Let's combine these two concepts of \textit{Gradient Boosting} with the \textit{Decision Trees} as base learners, as Friedman et al. \cite{Friedman2001} proposed in his paper. \\
For simplification we consider the special case where each base learner is an $J$-terminal node regression tree \cite{Breiman1984}. Each base learner as regression tree model itself has now the additive form
\begin{equation}
    h(\vec{x};\{b_j,R_j\}_1^J) = \sum_{j=1}^J b_j \mathds{1}(\vec{x} \in R_j)
\end{equation}
Here $\{R_j\}_1^J$ are again the disjoint regions that collectively cover the space of all joint values of the predictor variables $\vec{x}$ \cite{Friedman2001}. These regions are represented by the terminal nodes of the corresponding tree. The indicator function $\mathds{1}$ has the value 1 if its argument is true and zero otherwise. The ``parameters'' of this base learner are the coefficients $\{b_j\}_1^J$ and the quantities that define the boundaries of the regions $\{R_j\}_1^J$. These are the splitting variables and the values of those variables that represent the splits at the non-terminal nodes of the tree. Because the regions are disjoint that is equivalent to the prediction rule: if $\vec{x} \in R_j$
then $h(\vec{x}) = b_j$.
For a regression tree the updating process of $F_m$ in the GB-algorithm [Alg. \ref{alg: gb_algo}] becomes
\begin{equation}
    F_m(\vec{x}) = F_{m-1}(\vec{x}) + \rho_m \sum_{j=1}^J b_{jm} \mathds{1}(\vec{x} \in \mathbb{R}_{jm})
\end{equation}
The regions $\{R_{jm}\}_1^J$ are here defined by the terminal nodes of the tree at the $m$-th iteration. They are constructed to predict the pseudo-responses $\{\tilde{y}\}_1^N$ by least-squares [Alg. \ref{alg: gb_algo} (line 3)]. Considering the parameters $\{b_{jm}\}$ as the corresponding least-squares coefficients, which come from the optimization in the individual nested regions (\ref{eq: dt_metric}):
\begin{center}
    $b_{jm} = \mathbb{E}_{\vec{x}_i \in R_{jm}}[\tilde{y}_i]$
\end{center}
The scaling factor $\rho_m$ is again the solution to the ``line search'' [Alg. \ref{alg: gb_algo}].
The forward iteration can be also expressed as
\begin{equation}
        F_m(\vec{x}) = F_{m-1}(\vec{x}) + \sum_{j=1}^J \gamma_{jm} \mathds{1}(\vec{x} \in R_{jm})
\end{equation}
with the coefficients $\gamma_{jm} = \rho \cdot b_{jm}$. This can be viewed as adding $J$ separate basis functions at each step $\{\mathds{1}(\vec{x} \in R_{jm})\}_1^J$ instead of a single additive one. By using the optimal coefficients for each of these separate basis functions  one can further improve the quality of the fit.
These coefficients are the solution to
\begin{center}
    $\{\gamma_{jm}\}_1^J = \underset{\{\gamma_{jm}\}_1^J}{\arg\min} \sum_{i=1}^N L\left(y_i,F_{m-1}(\vec{x}_i) + \sum_{j=1}^J \gamma_{jm} \mathds{1}(\vec{x} \in R_{jm})\right)$
\end{center}
\textit{Owing to the disjoint nature of the regions produced by regression trees this reduces to,}
\begin{equation}
        \label{eq: gamma_jm}
        \gamma_{jm} = \underset{\gamma}{\arg\min} \sum_{\vec{x}_i \in R_{jm}} L\left(y_i,F_{m-1}(\vec{x}_i) + \gamma\right)
\end{equation}
as Friedman \cite{Friedman2001} derived. \textit{This is just the optimal constant update in each terminal node region, based on the loss function $L$ given the current approximation $F_{m-1}(\vec{x})$}. For the case of using LSD (least-squares deviation) regression (\ref{eq: gamma_jm}) becomes:
\begin{center}
    $\gamma_{jm} = \mathbb{E}_{\vec{x}_i \in R_{jm}}[y_i - F_{m-1}(\vec{x}_i)]$
\end{center}
Which is simply the arithmetic mean of the current residuals in the $j$-th terminal node at the $m$-th iteration. At each iteration a regression tree is built to best predict the sign of the current residuals $[y_i - F_{m-1}(\vec{x}_i)]$ based on a least-squared criterion. Then the approximation is updated by adding the \textit{arithmetic mean} of the residuals in each of the derived terminal nodes.
\\
\begin{algorithm}
\caption{LSD Tree-Boost}
\label{alg: lsd_tree_boost_algo}
    $F_0(\vec{x}) = \text{mean}\{y_i\}_1^N$ \\
    \For{$m = 1$ to $M$}
    {
    $\tilde{y}_i = \sum_{i=1}^N [y_i - F_{m-1}(\vec{x})]^2$ \\
    $\{R_{jm}\} = J-\text{terminal node tree}(\{\tilde{y}_i,\vec{x}_i\}_1^N)$ \\
    $\gamma_{jm} = \mathbb{E}_{\vec{x}_i \in R_{jm}}[y_i - F_{m-1}(\vec{x}_i)] \text{ for } j = 1,..,J$ \\
    $F_m(\vec{x}) = F_{m-1}(\vec{x}) + \sum_{j=1}^J \gamma_{jm} \mathds{1}(\vec{x} \in R_{jm})$
    }
\end{algorithm}
The trees use only order information on the individual input variables $\vec{x}_j$ and the pseudo-responses $\tilde{y}_i$ have a discrete set of values. Therefore the terminal node updates are based on the mean of sample points in the respective region.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Regularization}
When predicting an output through fitting the training data too closely one can get a problem with overfitting the model. \textit{Reducing the expected loss on the training data beyond some point causes the prediction-expected loss to stop decreasing and often to start increasing again.
Regularization methods attempt to prevent such ``overfitting'' by constraining the fitting procedure inherently}, as Friedman et al. \cite{Friedman2001} proposed originally.
For additive expansions an inherent natural regularization parameter is the number of estimators or components $M$. This is similar to stepwise regression where the $\{h(\vec{x};\vec{a}_m)\}_1^M$ are considered explanatory variables that are sequentially entered. Through the adjustment of the number of estimators (= base learners) $M$ the regularization degree for a specific expected loss on the training data can be minimized. The best value for $M$ can be estimated by some model selection method such as cross-validation (CV) by \textit{Gridsearch}- or \textit{Bayesian-Optimization}. \\
\textit{Regularizing by controlling the number of terms in the expansion places an
implicit prior belief that ``sparse'' approximations involving fewer terms are
likely to provide better prediction. However, it has often been found that regularization through shrinkage provides superior results to that obtained by restricting the number of components \cite{Copas1997}}, as Friedman \cite{Friedman2001} described.
When working with additive models in a forward stagewise manner a simple shrinkage strategy is to replace the last line of the generic algorithm [Alg. \ref{alg: gb_algo}] with
\begin{equation}
    \label{eq: gb_algo_shrinkage}
    F_m(\vec{x}) = F_{m-1}(\vec{x}) + \nu \cdot \rho_{m} h(\vec{x};\vec{a}_m) \qquad 0 < \nu \leq 1
\end{equation}
(equally for all other stated algorithms). Consequentially each update is simply scaled down by the value of the ``learning rate'' parameter $\nu$. This \textit{Bias-Variance trade-off} is a fundamental concept at machine learning tasks which describes the capability of the model to analyze the underlying patterns (low bias) and sensitivity of noise (low variance) in the data. So introducing shrinkage into Gradient Boosting (\ref{eq: gb_algo_shrinkage}) in this manner provides two regularization parameters, the learning rate $\nu$ and the overall number of components $M$. Each one can control the degree of fit and thus affect the best value for the other one. Decreasing the value $\nu$ increases the best value for $M$. Ideally one should estimate optimal values for both by minimizing a model selection criterion jointly with respect to the values of the two parameters. But one has to consider the proportional increase in computation time when increasing the amount base learners $M$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Cross validation}
\label{sec: cross_validation}
Cross validation or out-of-sample testing is a flexible model validation technique for evaluating how the results of a statistical analysis will generalize to an independent data set. It is a resampling method that uses different portions of the data to test and train a model in an iterative manner. It is mainly used in settings where the goal is prediction (inference) and one wants to estimate how accurately a predictive model will perform in practice. Usually one has a dataset of \textit{known data} on which training is run (training dataset) and a dataset of \textit{unknown data} against which the model is tested (validation dataset).
Therefore the goal of cross-validation (CV) is to test the model's ability to predict new data that was not used in estimating it in order to flag problems like overfitting or selection bias. \\
One round of CV involves partitioning a sample of data into complementary subsets (training and validation subset), performing the analysis on the training subset and validating the analysis on the validation subset. To reduce variability in most methods multiple rounds of cross-validation are performed using different partitions. So to sum up, CV averages over different data partitions and measure the fit quality in prediction to derive a more accurate estimate of model prediction performance via a specific metric. \\
Generally there are two types of CV methods which have to be distinguished: \textit{exhaustive} and \textit{non-exhaustive} strategies. An exhaustive cross-validation method learns and tests on all possible ways to divide the original sample into training and validation set. Non-exhaustive cross validation methods do not compute all ways of splitting the original sample. These methods are approximations of leave-$p$-out cross-validation.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{$k$-fold cross-validation}
In $k$-fold cross-validation (non-exhaustive) the original sample is randomly partitioned into $k$ equal size sub-samples.
Of the total $k$ sub-samples, a single sub-sample is retained as the validation data for testing the model and the remaining $k - 1$ sub-samples are used as training data. This cross validation process for evaluating the best hyper-parameters is then repeated $k$ times with each of the $k$ sub-samples used exactly once as the validation data. Finally the $k$ results can then be averaged to produce a single estimation. One advantage of this method over repeated random sub-sampling is that all observations are used for both training and validation and each observation is used for validation exactly once. In the figure below is a typical flowchart visible for model training [Fig. \ref{fig: cv_flowchart}].
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.45\textwidth,trim={0 0 0 0},clip]{figures/bs_ml_figure_3.png}
    \caption[Flowchart of model training]{Flowchart of model training and testing via cross-validation (Ref. \cite{Scikit2023})}
    \label{fig: cv_flowchart}
\end{figure}
\\
The dataset gets split up for training and testing (validation) the model. By alternately using those sets [\ref{fig: cv_split}] one can obtain the best parameter constellation via the cross-validation metric. With the retrained model on the best hyper-parameters based on some scoring metric one can finally use the trained model to predict on new data. \\
In most data science problems the initial dataset consists only of training data. The model is trained using $k - 1$ of the folds as training data and the resulting model is validated on the remaining part of the training data [Fig. \ref{fig: cv_split}].
The performance measure reported by $k$-fold cross validating is then the average of the values computed in the loop.
After initializing the hyper-parameters of the framework one can cross-validate the training set and choose more suitable ones by comparing the models. Usually the best parameters for a given Gradient boosting framework are determined by uninformed Gridsearch- or informed Bayesian optimization frameworks.
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{figures/bs_ml_figure_4.png}
    \caption[Gridsearch by cross validation parameter evaluation]{Gridsearch by $k$-cross validation parameter evaluation (Ref. \cite{Scikit2023})}
    \label{fig: cv_split}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Uninformed cross-validation / Gridsearch or Random Search}
Given a boosting framework with hyperparameters $\{\theta_1,...,\theta_n\}$ to tune one can use the uninformed gridsearch framework to determine the highest score of performance of the underlying model. \\
The gridsearch method spans an $n$-dimensional space of the given hyperparameters with predefined boundaries of the given parameter interval and exhaustively generates candidates from a grid of parameter values specified.
The gridsearch cross validation framework implements the usual estimator: when ``fitting" it on a dataset all the possible combinations of parameter values are evaluated and the best combination is retained. It has to be said that is not guaranteed to find the truly best hyperparameters if the discretization of the grid ($N$ grid nodes per intervals) is not high enough. Moreover the grid search scales as $N^n$, which becomes computationally prohibitive. This problem vanishes when using a randomized search cross validation framework. Random search tries randomly chosen combinations of a range of values where the number of iterations have to be defined beforehand. The problem here is that there's no guarantee that the output includes the best hyperparameter combination. Consequently one can say the gridsearch method is a directed method with predefined process flow in contrast to random search which randomly choose the grid point and do not learn from prior information. To prevent this an informed cross validation strategy can be used like Bayesian optimization.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Informed cross-validation / Bayesian Optimization}
\label{sec: bayesian_opt}
As stated in the uninformed parameter optimization approach, the idea is of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. This is similar to other optimization problems where one is interested in finding the minimum of a function $f(\vec{x})$ on some bounded set $\mathcal{X}$, which represents a subset of $\mathbb{R}^N$ where $N$ denotes the dimension of the hyperparameter-space. By constructing a probabilistic model for $f(\vec{x})$ and then exploiting this model to make decisions about where in $\mathcal{X}$ to next evaluate the function, while integrating out uncertainty. \\
The decisive advantage is to use \textit{all} of the information available from previous evaluations of $f(\vec{x})$ and not simply rely on local gradients \cite{Snoek2012}. This results in a procedure that can find the minimum of difficult non-convex functions with relatively few evaluations. \\
In general this tuning process can be viewed as the optimization of an unknown black-box function $f(\vec{x})$. For continuous functions, Bayesian optimization typically works by assuming the unknown objective function (``black box function'') was sampled from a Gaussian Process (GP) and maintains a posterior distribution for this function as observations are made \cite{Frazier2018}. \\
Since this objective function is unknown, the Bayesian strategy is to treat it as random function and place a prior over it. This prior captures beliefs about the behavior of the function. After gathering the function evaluations, which are treated as data, the prior is updated to form the posterior distribution over the objective function (Likelihood function). \\
Finally the posterior distribution is used to construct an ``acquisition function'' that determines the next query point for exploring the search space. The model used for approximating the objective function is called the \textit{surrogate model}.
Through this sampling schematic process the optimization problem gets computationally less expensive. \\
In the case of tuning the learning model, these observations are the measure of generalization performance under different settings of the hyperparameters we wish to optimize. For picking the ideal hyperparameters of the next iteration, one can optimize the expected improvment (EI) \cite{Mockus} over the current best result or the Gaussian process upper confidence bound (UCB) \cite{Srinivas2009}. EI and UCB have been shown to be efficient in the number of function evaluations required to find the global optimum of many multimodal (multiple solutions) black-box functions \cite{Srinivas2009, Snoek2012}.
\subsubsection{Bayesian Optimization with Gaussian Process Priors}
Two major choices must be made when performing Bayesian optimization. First, one must select a prior over functions (the \textit{surrogate model}) that will express assumptions about the function being optimized. For this one often choose the Gaussian process prior, due to its smoothness, flexibility and tractability. Additionally the tractable posterior distribution induced by the Gaussian prior is cheap to evaluate and leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next \cite{Snoek2012} where sampling is likely to yield an improvement. \\
Second, we must choose an \textit{acquisition function}, which is used to construct a utility function from the model posterior allowing us to determine the next point to evaluate. The pseudo-code for implementing Bayesian optimization is shown in [Alg. \ref{alg: bayesian_opt}] \cite{Frazier2018}.
\begin{algorithm}
\caption{Bayesian Optimization \cite{Frazier2018}}
    \label{alg: bayesian_opt}
    Place a Gaussian process prior \\
    Observe $f$ at $n_0$ points according to an initial space-filling experimental design. \\
    Set $n = n_0$ \\
    \While{$n \leq N$}
    {
    Update the posterior distribution on $f$ using all available data \\
    Let $x_n$ be a maximizer of the acquisition function over $x$, where the acquisition function is computed using the current posterior distribution.\\
    Observe $y_n = f(x_n)$ and incrementally increase $n$.
    }
    \textbf{Return} a solution: either the point evaluated with the largest $f(x)$ or the point with the largest posterior mean.
\end{algorithm}
\newpage
So first a surrogate model and an acquisition function has to be initiated. Then for each iteration the hyperparameter $x_n$ has to be found where the acquisition function is maximized (or minimized, depending on the used acquisition function). Then the objective function score of $x_n$ has to be obtained to see how this points actually performs. By including the new hyper-parameter $x_n$ and the true objective function score in the history of other samples one can train again the surrogate model using the last history.
%\newpage
When using Gaussian processes in this ``sequential'' approach the posterior probability is modeled directly:
\begin{center}
    $p(y|x) = \frac{p(x|y) \cdot p(y)}{p(x)}$ \\
    with $ p(y|x) = p(\text{true objective function} | \text{hyperparameter})$
\end{center}
\subsubsection{Gaussian Processes}
The Gaussian process (GP) is a convenient and powerful prior distribution over functions which be of the form $f : \mathcal{X} \rightarrow \mathbb{R}$. A Gaussian Process is defined by the property that any finite subset of $N$ points $\{ \vec{x}_n \in \mathcal{X}\}_{n=1}^N$ induces a multivariate Gaussian distribution in $\mathbb{R}$. The $n$-th of these points is taken to be the function value $f(\vec{x}_n)$ and the elegant marginalization properties of Gaussian distributions allow us to compute marginals and conditionals in closed form \cite{Snoek2012}. The support and properties of the resulting distribution on functions are determined by a mean function $m : \mathcal{X} \rightarrow \mathbb{R}$ and a positive semi-definite covariance function $K : \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$.
\subsubsection{Acquisition Functions for Bayesian Optimization}
Let's assume that the function $f(\vec{x})$ is drawn from a Gaussian process prior and that our observations are of the form $\{\vec{x}_n,y_n\}_{n=1}^N$, where $y_n \approx \mathcal{N}(f(\vec{x}_n),\nu)$ and $\nu$ is the variance of noise introduced into the function observations.
This prior and the data specific noise induce a posterior over functions, which in turn induces the acquisition function, which we denote by a: $\mathcal{X} \rightarrow \mathbb{R}^{+}$, determines what point in $\mathcal{X}$ should be evaluated next via a ``proxy'' optimization $x_{n+1} = \underset{\vec{x}}\arg\max a(\vec{x})$ \cite{Snoek2012}. So the acquisition functions propose the next sample points in the seach space. In general, these acquisition functions depend on the previous observations, as well as the learnable GP hyperparameters. From this we denote this dependence as $a(\vec{x};\{\vec{x}_n,y_n\},\theta)$, whereby the $\{\vec{x}_n,y_n\}$ tuple-set represents the previous sampling points and $\theta$ are the GP hyperparameters.
There are several popular choices of acquisition functions. Under the Gaussian process prior, these functions depend on the model solely through its predictive mean function $\mu(\vec{x};\{\vec{x_n},y_n\},\theta)$ and predictive variance function $\nu(\cdot) = \sigma^2(\vec{x};\{\vec{x}_n, y_n\}, \theta)$.\\
Moreover one has to mention the trade off between exploitation and exploration.\\
\textit{Exploitation} means sampling where the surrogate model predicts a high objective and \textit{exploration} means sampling at locations where the prediction uncertainty is high. Both effects correspond to high acquisition function values and the overall goal is to maximize the acquisition function to determine the next sampling point. There are several types of acquisition functions which are commonly used: \\
\\
\textbf{Probability of Improvement} \\
The most intuitive strategy is to maximize the probability of improvement (POI) over the best current value \cite{Kushner1964}. By using Gaussian Procceses as prior this can be computed analytically in the following way, whereby $\phi(\cdot)$ just denotes the probability density function:
\begin{equation}
    a_{PI}(\vec{x};\{\vec{x}_n ,y_n\},\theta) = \phi(\gamma(\vec{x}))
    \qquad
    \gamma(\vec{x}) = \frac{f(\vec{x}_{best}) - \mu(\vec{x}; \{\vec{x}_n,y_n\},\theta)}{\sigma(\vec{x};\{\vec{x}_n,y_n\},\theta)}
\end{equation}
\textbf{Expected Improvement} \\
Alternatively, one could choose to maximize the expected improvement (EI) over the current best. Where Probability of Improvement considers only the probability of improving our current best estimate, Expected Improvement factors in also the magnitude of the actual improvement. This has also closed form under the Gaussian process:
\begin{equation}
\label{eq: ei_short}
    a_{EI}(\vec{x};\{\vec{x}_n ,y_n\},\theta) = \sigma(\vec{x};\{\vec{x}_n ,y_n\},\theta) \cdot
    [\gamma ( \vec{x} ) \phi(\gamma(\vec{x}))
    + \mathcal{N}(\gamma(\vec{x};0,1))]
\end{equation}
\textbf{Upper Confidence Bound}\\
A more recent development is the idea of exploiting lower confidence bounds (upper, when considering maximization) to construct acquisition functions that minimize regret over the course of their optimization \cite{Srinivas2009, Snoek2012}. Using such framework requires acquisition functions of the form
\begin{equation}
    a_{UCB}(\vec{x};\{\vec{x}_n ,y_n\},\theta) = \mu(\vec{x};\{\vec{x}_n ,y_n\},\theta)
    \pm \kappa \sigma(\vec{x};\{\vec{x}_n ,y_n\},\theta)
\end{equation}
with a tune-able $\kappa$ to balance exploitation against exploration.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Expected Improvement in detail}
Because of the implementation by scratch we'll look more under the hood how the Expected Improvement acquisition function works. So instead of looking only at the improvement $I(x)$, which is a random variable, we will instead calculate the ``Expected Improvement'', which is consequently the expected value of $I(x)$.
For simplification we'll denote $\gamma(\vec{x})$ from beforehand to $z$ and $\vec{x}_{best}$ to $x^{\ast}$ (objective function with scalar input). Dependencies will also be neglected if not necessarily needed.
Based on notation the expected value is a generalization of the weighted average, therefore:
\begin{equation}
    \text{EI}(x) \equiv \mathbb{E}[I(x)] = \int_{- \infty}^{+ \infty} I(x) \phi(z) dz =
    \int_{- \infty}^{+ \infty} \underbrace{ {\max}[f(x) - f(x^{\ast}),0] }_{I(x)} \phi(z) dz
\end{equation}
Where $\phi(z)$ is the probability density function (PDF) of the normal distribution $\mathcal{N}(0,1)$.
\begin{center}
    $\phi(z) = \frac{1}{\sqrt{2\pi}} \text{exp}(-z^2/2)$
\end{center}
To solve this integral the $\max$ operator must be vanished. One elegant way is to split up the integral into two components depending one where $f(x) - f(x^{\ast})$ gets positive or negative. This switching point over the domain is given by:
\begin{center}
    $f(x) = f(x^{\ast}) \rightarrow \mu + \sigma z = f(x^{\ast}) \rightarrow z_0 = \frac{f(x^{\ast}) - \mu}{\sigma}$
\end{center}
By setting this splitting point as intermediate boundary for the integration the Expected Improvement results to:
\begin{equation}
    \text{EI}(x) = \underbrace{\int_{- \infty}^{+ z_0} I(x) \phi(z) dz}_{\text{zero since }  I(x) = 0} + \int_{z_0}^{+ \infty} I(x) \phi(z) dz
\end{equation}
By plugging in $f(x) = \mu + \sigma z$ into the second integral one gets:
\begin{equation}
    \text{EI} = \int_{z_0}^{+ \infty} [\mu + \sigma z - f(x^{\ast})] \phi(z) dz
\end{equation}
Because we're integrating over $z$ we can exclude the independent terms from integration and plug in the PDF of the normal distribution:
\begin{equation}
    \text{EI} = (\mu - f(x^{\ast})) \int_{z_0}^{+ \infty} \phi(z) dz + \frac{\sigma}{\sqrt{2\pi}}\int_{z_0}^{+ \infty} z \cdot \text{exp}(-z^2/2) dz
\end{equation}
The first integral results to [$1 - \Phi(z_0)$] (where $\Phi(z_0)$ is the CDF) and the second term can be simplified by using the derivation of the integration term.
\begin{equation}
    \text{EI} = (\mu - f(x^{\ast})) (1 - \Phi(z_0)) - \frac{\sigma}{\sqrt{2\pi}} [\text{exp}(-z^2/2)]_{z_0}^{+ \infty}
\end{equation}
The shift in CDF [$1 - \Phi(z_0)$] results to $\Phi(- z_0)$ and because of symmetry of the normal distribution it's consequently $\Phi(z_0)$. The second term shifts to the PDF of $\phi(z_0)$ because on the infinite boundary it converges to zero.
Therefore one get the closed form of the Expected Improvement which is equivalent as the formula stated in (\ref{eq: ei_short}):
\begin{equation}
\label{eq: ei_raw}
    \text{EI} = [\mu - f(x^{\ast})] \Phi \left(\frac{\mu - f(x^{\ast})}{\sigma}\right) + \sigma \phi \left (\frac{\mu - f(x^{\ast})}{\sigma} \right)
\end{equation}
So EI$(x)$ take high values when:
\begin{itemize}
    \item $\mu > f(x^{\ast})$ ... mean value of the Gaussian Process is high at $x$
    \item $\sigma(x) > 1$ ... increase in uncertainty
\end{itemize}
Additionally if $\sigma(x) = 0$ then EI$(x) = 0$.
One can also inject a hyper-parameter $\epsilon$ into the equation (\ref{eq: ei_raw}) to tune how much exploitation versus how much exploration the BO algorithm will perform.
\begin{equation}
\label{eq: ei_hp}
        \text{EI} = [\mu - f(x^{\ast}) - \epsilon] \Phi\left(\frac{\mu - f(x^{\ast} - \epsilon)}{\sigma} \right) + \sigma \phi\left(\frac{\mu - f(x^{\ast}) - \epsilon}{\sigma}\right)
\end{equation}
One has to keep in mind that the first summation term in equation (\ref{eq: ei_hp}) is the exploitation term and second is the exploration term. Increasing $\epsilon$ will decrease the importance of improvements predicted by the GP posterior mean $\mu(x)$ relative to the importance of potential improvements in regions of high prediction uncertainty, represented by large $\sigma(x)$ values.
\begin{comment}
\subsubsection{Sequential Domain Reduction}
Sequential Domain Reduction is a method where the bounds of the optimization problem are mutated (typically contracted) to reduce the time required to converge to an optimal value. It is often used when the cost function is particularly expensive to calculate or if the optimization routine oscillates heavily. Because one have to keep in mind the high-dimensional search-space with possible multiple maxima's of the functions hyper-surface.\\
The basic steps are \textit{panning} and \textit{zooming}. These two steps are applied at one time therefore updating the problem search space every iteration. By \textit{panning} the region of interest will be recentered around the most optimal point which have been found and through \textit{zooming} the region of interest will be contracted, as \cite{Stander2002} stated the underlying methodology of \textit{Successive Respone Surface Method (SRSM)} algorithm.
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.7\textwidth,trim={0 0 0 0},clip]{figures/sdr.png}
    \caption[Adaption of subregion / Sequential Domain Reduction]{Sequential Domain Reduction \cite{Stander2002} / a) panning b) zooming c) panning \& zooming}
    \label{fig: sdr_schematic}
\end{figure}
\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}

\newpage
\section{Principal component analysis (PCA)}
\label{sec: pca}
Principal component analysis (PCA) forms the basis of \textbf{multivariate data analysis} based on projection methods. For tuning the hyperparameters via Bayesian optimization the dataset will be dimensionality reduced for accelerating the optimization task. \\
Computing a PCA of a data set amounts to constructing a singular value decomposition (SVD) that accurately approximates the matrix $A$ containing the data being analyzed.
For high dimensional datasets Halko et al. [Ref. \cite{Halko2011}] proposed a new algorithm which is implemented in the Scikit [Ref. \cite{Scikit2023}] Python API package which is used.
For completeness the algorithm sketch is stated here shortly: \\
That is, if $A$ is $m \times n$, then we must find a positive integer $k < \min(m,n)$ and construct matrices $U, \Sigma$ and $V$ such that
\begin{equation}
    A \approx U \Sigma V^T
\end{equation}
with $U$ being an $m \times k$ matrix whose columns are orthonormal, $V$ being an $n \times k$ matrix whose columns are orthonormal and $\Sigma$ being a diagonal $k \times k$ matrix whose entries are all positive.
The idea is to construct a low-rank (say, rank $k$) approximation of $U \Sigma V^T$ to any given real matrix $A$, such that
\begin{equation}
    ||A - U \Sigma V^T||_2 \leq (Ckn)^{1/(4i+2) \sigma_{k+1}}
\end{equation}
with very high probability ($\approx 1$, independent of $A$) where $m$ and $n$ are the dimensions of the given $m \times n$ matrix $A$ whereby $\sigma_{k+1}$ is the $(k + 1)^{st}$ greatest singular value of $A$ and $C$ is a constant determining the probability of failure. The $|| \cdot ||_2$ is the spectral ($l^2$-operator) norm which can be written as
\begin{center}
    $
    ||A - U \Sigma V^T||_2 = \underset{x \in \mathcal{R}^n : ||x||_2 \neq 0}{\arg\max} \frac{||A - (U \Sigma V^T) x ||_2}{||x||_2}
    \quad \text{with} \quad ||x||_2 = \sqrt{\sum_{j=1}^n(x_j)^2}.
    $
\end{center}
One can use the variance to measure the information of dataset but this will only make sense in relationship to some given model. Therefor one can use the given variance of input features $x_i$ for specifying the amount of information which gets lost when reducing the dimension of the given dataset. The information loss $IL(\vec{x})$ is therefore proportional to the variance $\nu$ explained by the singular values of the decomposition [Ref. \cite{Scikit2023}].
Whereby $\Sigma$ is the diagonal $k \times k$ matrix and $n$ is the number of samples.
\begin{center}
    $\nu_{j} = \frac{\Sigma^2}{(n - 1)}$ \\
    $IL(\vec{x}) \propto \nu_{SVR} = \frac{\nu_{j}}{\nu_{total}} = \frac{\frac{\Sigma^2}{(n - 1)}}{\sum_j^n \nu_j}$
\end{center}
Consequently $\nu_{SVR}$ is also called the explained ratio of variance of the decomposition and is now a ``measurement" of the information loss in our dataset
\end{comment}
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Testing the framework on a case study}
Let's suppose we have a scalar test function of the form
\begin{center}
    $F(x) = \text{sin}(x) + \text{ln}(x) + \mathcal{N}(\cdot)$
\end{center}
in the domain $x \in [0, 10]$ (200 sampling points) with a Gaussian noise $\mathcal{N}(\cdot)$ which implies some uncertainty in the function space.
This function behaves quite non-linear because of it's oscillating and logarithmic nature. \\
Nevertheless, fitting the data with a linear regression by least squares method, whereby the sum of squared residuals have the form $S = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$, resulting in a mean square of deviation $R^2 = 0{,}48$ over the given domain. Applying a \textit{chi-square test} sets the theoretical bounds for the confidence and prediction limits resulting in a weakly informative model.
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{figures/linear_regression.png}
    \caption[Linear Regression of the test function]{Linear Regression of the test function $F(x)$ with Gaussian Noise $\mathcal{N}(\mu_i = y_i, \sigma = 0{,}5)$ with applied chi-square test and resulting 95\% confidence interval and 95 \% prediction limits}
    \label{fig: linear_regression_testfunction}
\end{figure}
By implementing a \textit{Gradient Boosting Framework} with \textit{Decision Trees} as Base Learners one could benefit of the dynamic additive modeling with the non-linearly behaving base-learners.
Because we're dealing with a more or less ``continuous'' domain of our function a least-squares regression model will be implemented as stated in section [\ref{sec: regr_boosted_trees}]. The pseudo-code which was written in [Alg. \ref{alg: gb_ls_algo}] will be implemented with the appropriate decision tree design [Alg. \ref{alg: tree_growing_algo}]. The self-written code is available in the following Github repository \cite{Gschaider}.
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.95\textwidth,trim={0 0 0 0},clip]{figures/gbm_iterations.png}
    \caption[GBM framework iterations]{GBM framework with fixed depth of the decision tree regressor over iterations / left column shows the given residuals on each step and the generic function $h_m(x)$ which will be added to the model on each step / right column shows the sample points by the testfunction $F(x)$ and the predicted model $\tilde{F}_m(x) = \hat{y}$}
    \label{fig: gbm_iterations}
\end{figure}
We can see in figure [\ref{fig: gbm_iterations}] that in the beginning the model will build the mean of the input space $x$ and adding the generic functions $h_m(x)$ on each step / iteration. This is equivalent by summing all different trees up, therefor $m = n_{trees}$. Because we've limited the model only to 40 estimators and even it's predicting the initial function quite well the full potential will be revealed within the right choice of hyperparameters.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Gradient Boosting framework}
For analyzing the efficiency of the written algorithm the mean squared error (MSE) shorthanded as deviance $\Delta$ over the boosting iterations (total number of different added trees) will now be compared. For the gradient boosting machine from scratch (subindex $c$) we'll use the last deviance over the total amount of estimators. For comparison the official \textit{SKLEARN} framework will be used \cite{Scikit2023} (subindex $k$). We'll use the same test-function $F(x)$ as one dimensional regression problem. We can obtain that the convergence of the prediction of the trained GB models with $n_{trees} = 100$ and $d_{max}$ = 1 over the given sample points are nearly identical with values of $\Delta_{k/c} \approx 0{,}208$  (depending on the random noise $\mathcal{N}(\cdot)$) with a deviation of $10^{-7}$ between both models.
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{figures/gbm_scratch_sklearn_loss_comparison.png}
    \caption[GBM framework comparison]{GBM framework comparison with fixed hyperparameters / from the selfwritten algorithm (scratch) with SKLEARN \cite{Scikit2023}}
    \label{fig: gbm_comparison}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Decision Tree Regressor framework}
Suppose we would not use additive modeling for predicting the test-function $F(x)$ but rather using only one single decision tree regressor for splitting the input domain. This would result in the same solution as using only one estimator ($m$ = 1). \\
Therefor we could determine the best splitting point through calculating the mean square error on each sub-domain of the inputs features as stated in section [\ref{sec: decision_trees}]. One could argue that using only one splitting function would give a solid approximation over the given sample points. Doing this for a maximal tree depth of $d_{max} = 1$ gives the following approximation $\tilde{y}$ of one estimator ($m = 1$) or multiple estimators ($m = 1000$) [Fig. \ref{fig: gbm_dt_comparison}].
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{figures/gbm_decision_tree_comparison.png}
    \caption[GBM-DT function prediction comparison over estimators]{GBM DT function prediction comparison for $m = 1$ or $m = 1000$ estimators (= number of trees $n_{trees}$)}
    \label{fig: gbm_dt_comparison}
\end{figure}
\\
Here it's identifiable that the key advantage comes definitely within the additive modeling schematic. The decision tree regressor brings non-linearity whereby the number of estimators brings contour (shape in a sense that the function behavior can be characterized) which characterizes this specific ensemble technique called boosting. \\
Taking a closer look in the decision tree splitting criteria over the test-function gives following approximations with tree structures $d_{max} \in \{1,2\}$ for the given data-set $x \in \mathcal{X}$ of the test function $F(x)$. When using a maximum depth of $d_{max} = 1$, so only one layer, would set the splitting criteria as shown in figure [\ref{fig: dt_comp_1} \& \ref{fig: dt_split_1}].
\begin{figure}[htbp]
\begin{minipage}[t]{8cm}
\vspace{0pt}
\centering
\includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{figures/decision_tree_regressor_comparison_md=1.png}
\caption[Decision Tree Regressor / One Layer]{Decision Tree Regressor / Sample points and split. function / $d_{max} = 1$}
\label{fig: dt_comp_1}
\end{minipage}
\hfill
\begin{minipage}[t]{8cm}
\vspace{0pt}
\centering
\includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{figures/decision_tree_splitting_structure_md=1.png}
\caption[Decision Path / One Layer]{Decision Path / $d_{max} = 1$}
\label{fig: dt_split_1}
\end{minipage}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
If $x_i$ is smaller than a threshold then $\tilde{y}$ gets assigned the mean:
\begin{center}
    $x_i \lessapprox 6{,}44 \rightarrow \tilde{y} \approx 0{,}90$ (left branch) $\vee$ $x_i \gtrapprox 6{,}44 \rightarrow \tilde{y} \approx 2{,}58$ (right branch).
\end{center}
This would result in 128 samples on the left and 72 sample on the right side given the data-set with an one layer tree structure with in total 2 leaves. \\
By increasing the depth of the tree to $d_{max} = 2$ one get a system with 4 leaves and 3 optimal splitting rules as shown in figure [\ref{fig: dt_comp_2} \& \ref{fig: dt_split_2}].
\begin{figure}[htbp]
\begin{minipage}[t]{8cm}
\vspace{0pt}
\centering
\includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{figures/decision_tree_regressor_comparison_md=2.png}
\caption[Decision Tree Regressor / Two Layers]{Decision Tree Regressor / Sample points and split. function / $d_{max} = 2$}
\label{fig: dt_comp_2}
\end{minipage}
\hfill
\begin{minipage}[t]{8cm}
\vspace{0pt}
\centering
\includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{figures/decision_tree_splitting_structure_md=2.png}
\caption[Decision Path / Two Layers]{Decision Path / $d_{max} = 2$}
\label{fig: dt_split_2}
\end{minipage}
\end{figure}
\\
In addition the SKLEARN module \cite{Scikit2023} is also shown in the plots which results in nearly the same splitting rules (the predicted $\tilde{y}$ values from the scratch algorithm were shifted by $\Delta y = 0{,}1$ for visibility).
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Hyperparameter optimization}
The right hyper-parameters are key for finding the optimal function approximation that minimizes the given cost function the best in terms of a ``scoring metric". As stated in section [\ref{sec: cross_validation}] the uninformative gridsearch strategy evaluates all combinations of parameter constellations (grid points). By using two hyper-parameters for the GBM-DT model this would result in a two dimensional grid (surface). In the following figure [\ref{fig: gbm_hyper_matrix}] some combinations for the scalar test-function $F(x)$ prediction problem are shown.
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=1\textwidth,trim={0.5 0 0 0},clip]{figures/gbm_with_decision_tree_various_parameters.png}
    \caption[GBM-DT hyperparameter matrix]{GBM-DT hyperparameter matrix for maximal depth $d_{max}$ and number of trees $n_{trees}$ / inital sample points and fitted GBM-DT model}
    \label{fig: gbm_hyper_matrix}
\end{figure}
We can see that the tree depth increases the approximation complexity and results more likely in overfitting. This is because the model learns relations very specific to the particular sample and will prioritize the given depth information (noise) too high. Increasing the number of trees or boosting rounds $n_{trees} = m$ expand the additive modeling structure and takes more samples points into account. Because of the high dimensional hyper-parameter space the informed cross validation method (BO) for finding the optimal parameter configuration is used.
As described in section [\ref{sec: bayesian_opt}] the idea is to learn from a given parameter combination which has been tested on the model over the training set and predicting the next better combination.
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bayesian Optimization of a test objective function}
Firstly the Bayesian Optimization Framework will be analyzed from scratch to understand the working mechanics of this powerful tool. For testing the framework we'll treat a known analytical expression of the test objective function $Z(x)$ as ``black-box" function.
\begin{center}
    $Z(x) = - \text{sin}(5x) + e^{2(x-1)} - x^2 + 1{,}5 \cdot x + 10 + \mathcal{N}(\cdot)$
\end{center}
Furthermore a Gaussian noise $\mathcal{N}(\cdot)$ will be added to simulate a real-world ``objective -function" [Fig. \ref{fig: bo_objective}]. Whereby the Gaussian noise mean is once again the specific function value of the interval $\mu_i = y_i$ and the standard deviation is set to $\sigma = 0{,}3$.
\begin{figure}[htbp]
\begin{minipage}[t]{10cm}
\vspace{0pt}
\centering
\includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{figures/bayesian_optimization_from_scratch_objective.png}
\caption[Test Objective Function for Bayesian Optimization]{Test Objective Function $Z(x)$ for Bayesian Optimization with Gaussian Noise $\mathcal{N}(\mu_i = y_i, \sigma = 0{,}3)$}
\label{fig: bo_objective}
\end{minipage}
\hfill
\begin{minipage}[t]{6cm}
\vspace{0pt}
\centering
\includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{figures/covariance_function_matern_gaussian.png}
\caption[Covariance functions for BO kernel]{Gaussian and Matern covariance function ($\nu = 0{,}1$) comparison for BO kernel}
\label{fig: covariance_functions}
\end{minipage}
\end{figure}
Through sequential iteration we'll try to approximate this ``black-box" function through a Bayesian optimization framework with Gaussian Processes as surrogate model. Because of simplicity we'll deal with a one-dimensional input domain $x \in \mathcal{X}$.
As Frazier et al. [Ref. \cite{Frazier2018}] described GP take a significant role in optimization problems, therefore a specific covariance function is often used when dealing with noisy data, namely the ``Matern Kernel" [Fig. \ref{fig: covariance_functions}]. The reason is because one can control the smoothness over flexibility of the GP model if needed. \\
The goal is to find the global optimum of this objective function in a small number of steps. We'll set the bounds of the domain to $\mathcal{X} = [-2, 3]$ (discretization of $\Delta x = 0{,}01$) and the initial sample points to $\mathcal{X}_{inital} = \{-1{,}9;+2{,}2\}$.
The Bayesian optimizer we'll be implemented as described in section [\ref{sec: bayesian_opt}] with Expected Improvement as acquisition function and for optimization the Quasi-Newton method with defined bounds we'll be used [Code Ref. \cite{Gschaider}]. \\
The left plot [Fig. \ref{fig: bo_iterations}] shows the noise-free objective function, the surrogate function which is the GP posterior predictive mean, the 95\% confidence interval of the mean and the noisy samples obtained from the objective function so far. The right plot [Fig. \ref{fig: bo_iterations}] shows the corresponding acquisition function at each iteration step and the vertical line in both plots shows the proposed sampling point for the next iteration which corresponds to the maximum of the acquisition function.
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{figures/bayesian_optimization_from_scratch_iterations.png}
    \caption[Bayesian Optimization iterations]{Bayesian Optimization iterations ($N=8$) with Gaussian Prior for predicting the Objective Function $Z(x)$ with Expected Improvement EI$(x)$ as acquisition function / the surrogate model is also visible with the 95\% confidence interval and the next sampling location}
    \label{fig: bo_iterations}
\end{figure}
\newpage
We can obtain the optimization process within the bounded region through maximizing the acquisition function. The surrogate model which gets build up by the additive priors through collecting the data by sampling through the regions of high interest. The next sampling location get proposed by the optimum of the acquisition function or by an increase in uncertainty (variance). \\
Interesting is here the balanced trade-off between exploration and exploitation. Initially the optimizer based on the two sample points drives the search direction to the right side based on the acquisition function. But exploration allows the algorithm to escape from this region and find the global optimum in the middle. So this trade-off by varying the exploration and exploitation parameters inherently is essential for finding the global maximum. But it has to be noted that by an increase of dimensions in the search space of hyperparameters this convergence can become computationally very expensive. Additionally the sampling point proposals often fall within regions of high uncertainty (exploration / iteration 4) and are not only driven by the highest surrogate function values (exploitation / like it is visible on iteration 3 or 5). After 8 iterations the global maximum of the objective function is found in this iteration round. It has to be said that the proposed sampling points for maximizing the acquisition function are drawn from a uniform distribution and therefore depending on the collected samples the convergence time depends on the specific iteration. \\
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{figures/bayesian_optimization_from_scratch_convergence_plot.png}
    \caption[Bayesian Optimization convergence plot]{Bayesian Optimization convergence plot of test-objective function $Z(x)$}
    \label{fig: bo_convergence_plot}
\end{figure}
In addition the convergence to the global optimum over the iterations is shown in figure [\ref{fig: bo_convergence_plot}]. On the first plot the distance $\Delta{x}$ between consecutive sample points $x_i$ is shown for attending the objective function maximum. On the second plot the resulting objective function values $\tilde{Z}(x)$ over the iterations is shown, which flattens out because consecutive sample points $x_i$ get narrower in the domain $\mathcal{X}$. It is immediately visible that the distance shrinks down with ongoing iterations, which is a good sign for finding the maximum. Nevertheless this convergence could theoretically be a local maximum and not the global but exploration / exploitation trade-off balances this out. For the plotting schematic a useful framework was used for simplification \cite{Pradeep}.
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Optimal Hyperparameter configuration for test function problem}
\label{sec: bayesian_opt_test}
Given the scalar testfunction from beforehand
\begin{center}
    $F(x) = \text{sin}(x) + \text{ln}(x) + \mathcal{N}(\cdot)$
\end{center}
we'll try to optimize the objective function $Z_F(\theta_i)$ over the hyperparameter space to get the best possible pair of parameters to fit our test function $F(x)$. The domain $\mathcal{X}$ will be discretized with 200 sampling points. We'll use the most inherent hyperparameters of Gradient Boosting Methods where the base learners are based on Decision Trees, as the proposed algorithm [Alg. \ref{alg: gb_ls_algo}].
\begin{itemize}
    \item Learning Rate $\nu$
    \item Maximal depth of trees $d_{max}$
    \item Number of Trees / Iterations $n_{trees} = m_{estimators}$
    \item Colsample by Tree $\epsilon$
\end{itemize}
Whereby we've introduced the learning rate $\nu$ as a parameter for shrinkage for regularization [\ref{eq: gb_algo_shrinkage}]. The maximal depth of trees $d_{max}$ resulted in the tree-structured design of our bases learners [\ref{alg: tree_growing_algo}]. Through the additive modeling schematic from our GB machine we'll use consequently the number of trees $n_{trees}$. Colsample by tree $\epsilon$ comes from \textit{Random Forests}. Because of the additive modeling behavior the model acts as ensemble of many trees. Therefor we could ``randomly" assign training samples to each tree (bootstrapping) and building each tree's nodes only considering a random subset of the attributes. So for each tree in a random forest one selects a random sample from the dataset to train the tree and for each node of this tree use a random subset of the features (if there are more than one).
So colsample by tree is the fraction of features selected for building a particular tree. The advantage is here to reduce overfitting and decorrelate the trees best possible.  \\
First the boundaries have to be set on each specific parameter to provide the optimizer the function space where the maximum should be found.
\begin{itemize}
    \item Learning Rate $\nu \in  [0{,}01 ; 0{,}2]$
    \item Maximal depth of trees $d_{max} \in [1; 10]$
    \item Number of Trees $n_{trees} \in [100; 20000]$
    \item Colsample by Tree $\epsilon \in [0{,}1; 1]$
\end{itemize}
Through the Bayesian Optimizer (BO) we'll predict where the global-maximum of our objective function $Z_F(\nu,d_{max},n_{trees},\epsilon)$ lies and therefor what the best configuration-set of hyperparameters are over the training set. Because of simplicity we'll use therefore the open-source built BO Python library, which Nogueira et al. \cite{Nogueira2014} developed. \\
The proposed acquisition functions in section [\ref{sec: bayesian_opt}] are all built-in and there's the possibility to iterate through the Bayesian prediction process. Two main parameters have to be chosen:
\begin{itemize}
    \item $n_{iter}$ : Number of steps of the Bayesian Optimization process
    \item $init_{points}$ : Number of steps of random exploration one wants to perform (random exploration helps to diversify the exploration space)
\end{itemize}
Through sub-sampling the dataset via 5-fold cross validation (CV) strategy (scikit-learn library \cite{Scikit2023}) we can generalize the function to an independent dataset of CV metrics (targets) as functions of hyperparameters as discussed in section [\ref{sec: cross_validation}]. We can use this CV-metric to evaluate the scoring of our parameter set which we've optimized through predicting the objective function. We'll use the standard scorer function, the quadratic loss function $L_2$ for evaluation of the score metric.
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{figures/gbm_with_decision_tree_hyperparameters_scatter_plot.png}
    \caption[Bayesian Optimizations iterations]{BO of the objective function $Z_F(\theta_i)$ over the hyperparameters from the initial scalar-test function $F(x)$ with $n_{iter}$ = 10 and $init_{points}$ = 100}
    \label{fig: gbm_dt_scatter_plot}
\end{figure}
\\
We can extract that there's definitely a section in each hyperparameter domain where the objective function gets maximized. Nevertheless we can obtain that there are also local optima which may not result in the best possible solution. When dealing with hyperparameter optimization one can prevent this by:
\begin{itemize}
    \item use a wider search space over the hyper-parameter domains
    \item offer a wide field of random initialization samples
    \item prioritize high-impact hyperparameters like learning rate, maximal tree depth or the number of trees
    \item using an ensemble strategy for combining results over multiple optimization tasks
\end{itemize}
Therefore we may not get only one parameter-configuration which may fit our needs but either more. We can use multiple test runs to confirm our initial combination of parameters. The resulting tuple of hyperparameters which maximizes our objective function best possible are:
\begin{itemize}
    \item Learning Rate $\nu = 0{,}03$
    \item Maximal depth of trees $d_{max} = 1$
    \item Number of Trees $n_{trees} = 902$
    \item Colsample by Tree $\epsilon = 0{,}1$
\end{itemize}
The GBDT-model gets trained with the hyperparameters and results in following model $\tilde{y}(x)$ over the domain $\mathcal{X}$ [Fig. \ref{fig: gbm_dt_best_par}].
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.9\textwidth,trim={0 0 0 0},clip]{figures/gbm_with_decision_tree_best_parameters.png}
    \caption[Best parameter configuration at GBM with Decision Trees on testfunction]{Best parameter configuration at GBM with Decision Trees on testfunction $F(x)$ with sample points and predicted function $\tilde{y}(x)$}
    \label{fig: gbm_dt_best_par}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Real-world data problem}
Here it was set as a task to use the presented Gradient Boosting Machine framework based on Decision Trees to create a real-world machine learning model. It has to be said that even if the presented toolkit can be used very flexibly, the main usage is per se not a time series prediction problem (non-stationary). But because of the innovative approach which \textit{Numer.ai} makes with their platform and dataset I decided to go with it.
Other standard usages of such supervised learning algorithms would be the prediction of house prices based on some input parameters, state of health prediction of lithium-ion batteries or indeed forecasting a stock-price. \textit{Numer.ai} \cite{Numerai}, the first decentralized hedge-fund of it's kind, provides an anonymized normalized time-series table for free. This dataset will be used to try to predict various stock prices several weeks in the future based on a trained GBM-DT model to buy or sell them based on the generated signals from a live-dataset [\ref{fig: numerai_workflow}].
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.6\textwidth,trim={0 0 0 0},clip]{figures/bs_ml_figure_5.png}
    \caption[Numer.ai data tournament participation process flow and dataset structure]{Numer.ai data tournament participation process flow and dataset structure \cite{Numerai}}
    \label{fig: numerai_workflow}
\end{figure}
\subsection{Numer.ai dataframe}
The datasets consists of different encrypted \textit{id's} which corresponds to a stock at a specific time era of over more than 500 \textit{eras} (each era is exactly one week apart) with over 1000 \textit{features} and 20 different \textit{targets}. This results in single feature vectors with more than 5 million entries  [Fig. \ref{fig: numerai_dataset}].
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.6\textwidth,trim={0 0 0 0},clip]{figures/numerai_dataset.png}
    \caption[Numer.ai dataset structure]{Numer.ai dataset structure \cite{Numerai}}
    \label{fig: numerai_dataset}
\end{figure}
One feature describes a quantitative attribute of a stock at a specific time (fundamentals like price/earnings ratio, technical signals like moving averages, market data like short interest, secondary data like analyst rating..) \cite{Numerai}.
Feature values are binned into 5 equal bins: $\{0;1;2;3;4\}$ and are overall subdivided by \textit{Numer.ai} in 8 different groups [also seen in Fig. \ref{fig: per_era_correlation}]. \\
\\
The target represents an abstract measure of performance a fixed number of weeks into the future (20-day or 60-day stock market returns). Specifically, it is a measure of ``stock-specific" returns that are not ``explained" by broader trends in the market, country, sector, or well-known ``factors" \cite{Numerai}. Because there are different target values provided one can build an ensemble model containing different sub-models which are trained on different targets. This is possible because the different target values are of the same unit in a sense. Some of them are heavy and other less correlated [Fig. \ref{fig: numerai_targets_correlation_matrix}]. \\
\\
Target values are binned into 5 unequal bins: $\{0; 0{,}25; 0{,}5; 0{,}75; 1{,}0\}$. This heavy regularization of feature and target values is to avoid overfitting as the underlying values are extremely noisy \cite{Numerai}. However, these bins do not mean that the problem is transformed to a classification, but rather a simplification of the possible values of features and targets. \\
This huge amount of data measures in raw CSV format around 10 Gigabyte of data and in the compressed \textit{parquet}-format around 1,8 Gigabyte. \\
\\
Additionally there is a validation dataset prepared for checking the model's performance before submitting a model [Fig. \ref{fig: numerai_workflow}]. By submitting a robust machine learning model based on the provided data one gets \textit{True Contribution} stake to overall meta-model which calculates the individual contribution. This \textit{Stake-Weighted Meta Model} produces trading signals, as Richard Craib (Founder) stated \cite{Craib2023}, whereby the \textit{Convex optimization machine} turns those signals into a portfolio by constructing risk factors [Fig. \ref{fig: numerai_true_contribution}]. Finally each week a portfolio is built with specific long and short positions built by a combination of all submitted machine learning models by the participants. \\
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.8\textwidth,trim={0 0 0 0},clip]{figures/true_contribution.png}
    \caption[Numer.ai True Contribution schematic]{Numer.ai True Contribution / sub-model to meta-model information flow \cite{Numerai, Craib2023}}
    \label{fig: numerai_true_contribution}
\end{figure}
\newpage
But because of the original scope of this bachelor thesis there's no additional room to go in depth of the working mechanics of the system behind \textit{Numer.ai} \cite{Numerai} but rather focusing on the data science problem. \\
\\
So to sum up, the data is structured in the form of a matrix $\hat{\mathcal{X}}$ (tabular data) whereby this matrix consists of multiple features along a ``time'' axis over multiple different objects, called the \textit{id's}. In stochastically behaving systems like the finance market models often tend to overfit and interpret ``noise'' as ``signal''. Therefor multiple target values are provided for building ensemble models, which are less sensitive to single exposures. Whereby the term \textit{exposure} comes from finance and means that a trading system being \textit{exposed to risk}. Consequently \textit{feature exposure} stands for the risk exposure of certain features. \\
\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Hardware requirements} \\
Because of the size of the dataframe the computations will take a certain amount of ressources and consequently running time. Therefor a linux based VPS was hosted with 8 vCPU-cores, 30 GB of RAM and 200 GB of SSD storage.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Statistical properties of features and targets}
\textit{Note:} Initially the dataset contained over 1000 features, whereby \textit{Numer.ai} released a new dataset in November 2023 with the small, medium or whole feature group. From now on the \textit{medium dataset} with 583 features was used because of computer resources.
There are also two different blocks for the targets: 20-day or 60-day stock market returns. Here only the 20-day rolling returns (four-week prediction) will be used (this results in 25 different targets).
\subsubsection{Mean and variance of features and targets}
All of the values in the dataframe are already normalized, so they have a fixed range of $x_i \in [0,5]$ with a mean of $\mu(x) \approx 2{,}0$ [Fig. \ref{fig: features_mean}] and most of the features ($>$ 80 \%) have a variance of $\nu(x) \approx 2{,}0$ [Fig. \ref{fig: features_variance}].
\begin{figure}[htbp]
\begin{minipage}[t]{8.8cm}
\vspace{0pt}
\centering
\includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{figures/train_df_features_mean_horizontal_barplot_2023-11-18.png}
\caption[Mean of Features]{Mean $\mu(\cdot)$ of features $\vec{x}_i$}
\label{fig: features_mean}
\end{minipage}
\hfill
\begin{minipage}[t]{8.8cm}
\vspace{0pt}
\centering
\includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{figures/train_df_features_variance_horizontal_barplot_2023-11-18.png}
\caption[Variance of Features]{Variance $\nu(\cdot)$ of features $\vec{x}_i$ }
\label{fig: features_variance}
\end{minipage}
\end{figure}
From the following plots one can obtain that all features exhibit a variance $\nu(\cdot)$ which results consequently in a non-uniform distribution of the features.
Switching from the feature to the target statistics one get similar results. The provided targets are binned to $y_i \in [0,1]$ with a mean of $\mu(y) \approx 0{,}5$ [Fig. \ref{fig: targets_mean}] and most of the targets ($>$ 80 \%) have variance of $\nu(y) \approx 0{,}05$ [Fig. \ref{fig: targets_variance}].
\begin{figure}[htbp]
\begin{minipage}[t]{8.5cm}
\vspace{0pt}
\centering
\includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{figures/train_df_targets_mean_horizontal_barplot_2023-11-18.png}
\caption[Mean of Targets]{Mean $\mu(\cdot)$ of targets $y_i$}
\label{fig: targets_mean}
\end{minipage}
\hfill
\begin{minipage}[t]{8.5cm}
\vspace{0pt}
\centering
\includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{figures/train_df_targets_variance_horizontal_barplot_2023-11-18.png}
\caption[Variance of Features]{Variance $\nu(\cdot)$ of targets $y_i$ }
\label{fig: targets_variance}
\end{minipage}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection{Correlation heatmaps of features and targets}
The correlation matrix of the features show that some features are indeed heavily correlated and some not [Fig. \ref{fig: numerai_features_correlation_matrix}]. Therefore it can make sense to exclude heavily correlated features in the beginning. But because of the additive modeling schematic over different base learners one gets immediately the induced weights of informative versus non-informative features. So we'll neglect that part in the \textit{feature selection} process, which is in general a very important step when dealing with such large datasets.
Interesting to note are recurrent patterns in the correlation matrix which indicates similar feature behavior over time (eras) in a sense. Therefore \textit{Numer.ai} established those feature groups because of high correlation. One can think of the similar qualitative features as a bundle of features, namely the feature group. The depicted correlation heatmaps [Fig. \ref{fig: numerai_features_correlation_matrix}] are not sorted in terms of the feature goups).
\begin{figure}[htbp]
\begin{minipage}[t]{9cm}
\vspace{0pt}
    \centering
    \includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{figures/feature_correlations_matrix_2023-11-18.png}
    \caption[Numer.ai dataset features correlation matrix]{Numer.ai dataset features correlation matrix}
    \label{fig: numerai_features_correlation_matrix}
\end{minipage}
\hfill
\begin{minipage}[t]{9cm}
\vspace{0pt}
    \centering
    \includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{figures/target_correlations_matrix_2023-11-18.png}
    \caption[Numer.ai dataset targets correlation matrix]{Numer.ai dataset targets correlation matrix}
    \label{fig: numerai_targets_correlation_matrix}
\end{minipage}
\end{figure}
The correlation matrix of the targets also provides an insight for building the right ensemble model [Fig. \ref{fig: numerai_targets_correlation_matrix}]. Less correlated targets can indeed be useful for diversifying the prediction model in terms of feature exposure. Consequently the weights of the model will be distributed differently because of other feature importances. We'll use this for building an ensemble model to combine multiple model behaviors and to reduce risk. \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Cross Validation methods}
Because of the importance of Cross validation at supervised machine learning tasks we've to compare different methods for validating our model performance. We'll use therefore a model selection method from the Python library \cite{Scikit2023}. We'll use the classical $k$-fold cross-validation method ($k$ = 5) as explained in section [\ref{sec: cross_validation}]. Because of the structure of the dataframe $\hat{\mathcal{X}}$ in relation to time the parameter $k$ can be set to 5 without causing any illogical problems (one week represents one era). In the framework there's the possibility to use different sub-methods whereby we'll focus on 4 different strategies:
\begin{itemize}
    \item $5$-fold with No Shuffling and No Random States
    \item $5$-fold with Shuffling and No Random States
    \item $5$-Grouped-fold with Shuffling and No Random States
    \item $5$-splitted TimeSeries Groups
\end{itemize}
Whereby the TimeSeries cross validation method is slightly modified compared to the Scikit's module, because it has to use groups and respect the era boundaries \cite{Numerai}.
When comparing these four different CV-methods by the standard score metric ($L_2$) one get slightly different values:
\begin{itemize}
    \item $5$-fold with No Shuffling and No Random States: $CV_{score} \approx 0{,}051$
    \item $5$-fold with Shuffling and No Random States: $CV_{score} \approx 0{,}054$
    \item $5$-Grouped-fold with Shuffling and No Random States: $CV_{score} \approx 0{,}052$
    \item $5$-splitted TimeSeries Groups $CV_{score} \approx 0{,}045$
\end{itemize}
One would think when dealing with a TimeSeries-dataset that consequently the Time-Series-Split would result in the best score. The Time-Series-Split is a variation of $k$-fold which returns first $k$-fold as train-set and the $(k + 1)$-th fold for testing \cite{Scikit2023}. We can obtain that indeed the TimeSeries Group has the best score of $CV_{score} \approx 0{,}045$. However the low difference in total score of $\Delta CV_{score} \approx 0{,}01$ strengthens the universal application of $k$-cross validation. Therefore we'll stick to $5$-fold cross validation with No Shuffling and No Random States.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Hyperparameter selection and optimization}
\label{sec: bayesian_opt_numerai}
With the Bayesian Optimizer we'll try to predict based on the best cross-validation metric the best hyperparameter combination as described in section [\ref{sec: bayesian_opt_test}]. \\
Due to the size of the dataset the optimization process is computationally very expensive for a solid number of iterations.
For speeding up the optimization process one could dimensionally reduce the dataset with PCA (principal component analysis). The information loss $IL(\vec{x})$ which would occur would then be proportional to the variance of the principal components.
Under the assumption that the reduction process via PCA does not inherently change the modeling behavior and consequently in shape conservation of the objective function. This has been done for testing the framework (a reduction of the 583 features (medium dataset) to $n = 300$ principal components would result in an information loss of $IL(\vec{x}) \approx 0{,}044 \%$). Ultimately the optimization has been performed on the whole (medium) dataset.
To make the optimization process as efficient as possible the BO Python library \cite{Nogueira2014} will be used with Upper-Confidence-Bounds acquisition function (standard method in the used module) and $n_{iter} = 100$ iterations and $init_{points} = 10$ points for exploration. \\
\newpage
We'll use the same hyperparameter constellation as in the test-case problem but with slightly larger parameter domains because of the size of the dataset:
\begin{itemize}
    \item Learning Rate $\nu \in  [0{,}01 ; 0{,}2]$
    \item Maximal depth of trees $d_{max} \in [1; 10]$
    \item Number of Trees $n_{trees} \in [100; 50000]$
    \item Colsample by Tree $\epsilon \in [0{,}1; 1]$
\end{itemize}
This results in the following scatter-plot where all objective function values $Z_F(\theta_i)$ got plotted over the different hyperparameters $\theta_i$ which were exploited in the BO search process [\ref{fig: bo_numerai_scatterplot}]. To visualize the maximum of the objective function of the hyper-parameters in the colored scatterplot the maximum $Z_{F_{max}}(\theta_i)$ got shifted by $\Delta = 0{,}1$.
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{figures/bo_numerai_scatter_plot.png}
    \caption[Hyperparameter iterations from BO over Numer.ai dataset]{Scatterplot of Optimization iterations from BO over Numer.ai dataset}
    \label{fig: bo_numerai_scatterplot}
\end{figure}
\\
The best combination of hyperparameters which maximizes our objective function best possible over the \textit{Numer.ai} training's-dataset with $5$-fold-CV are the following:
\begin{itemize}
    \item Learning Rate $\nu = 0{,}02$
    \item Maximal depth of trees $d_{max} = 1$
    \item Number of Trees $n_{trees} = 6352$
    \item Colsample by Tree $\epsilon = 0{,}9$
\end{itemize}
(The runtime for the Bayesian Optimization task takes around 95 hours with the stated hardware.)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Ensemble model and target selection}
As already stated the idea is to create an ensemble model where mutltiple GBM-DT models get trained over different targets. This is possible due to a similar property of the target values, for example (hypothetically) one model gets trained versus the normalized \textit{market cap} and one model gets trained versus the \textit{sector performance} etc. \\
For building an ensemble model $F_E(\vec{x})$ one can evaluate following properties of every sub-model $F_i(\vec{x})$.
\begin{itemize}
    \item specific target $y_i$
    \item hyper-parameters $\theta_i$
    \item feature domain $\vec{x}_i$
    \item weighting $\omega_i$
\end{itemize}
For simplicity the hyper-parameters $\theta_i$ of every sub-model are selected equally. Also the feature domain is left unchanged, which means that all input 583 features are used. So the focus is set on the target selection and the weighting metric is discussed further below. \\
Because of computational efficiency the official LightGBM framework \cite{LightGBM} is used for modeling instead of the self written algorithm \cite{Gschaider}, but ultimately the underlying clockwork of the \textit{Gradient Boosting}-Framework with \textit{Decision Tree}-Regressor as base learners is ``nearly" identical (for those 4 hyperparameters). One has to mention that because of the huge development of the framework there's always the possibility to configure the base model or add additional hyperparameters. With the given prediction problem one has the possibility to train each model versus a different target value which are correlated at different scale. \\
In the following the GBM-DT model which is trained on a \textbf{specific single target} with the hyper-parameters which were determined through BO [Sec. \ref{sec: bayesian_opt_numerai}] is denoted as \textbf{GBM-DT$^{\ast}$}. \\
The main target which is used to calculate the model performance is currently \textit{target cyrus}. Evaluating the GBM-DT$^{\ast}$ model performance over all different targets results in a very solid performance of \textit{target nomi}, \textit{target ralph}, \textit{target bravo} and \textit{target victor} [Fig. \ref{fig: cum_corr_val_preds_all_targets}]. \\
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=1\textwidth,trim={70 0 70 0 0},clip]{figures/2023-11-19_round0_all_targets_cumulative_correlation_of_validation_predicitions.png}
    \caption[Cumulative per era correlation of validation predictions over the GBM-DT$^{\ast}$ models over all different targets]{Cumulative per era correlation $\Sigma_i \text{corr}(\tilde{y}_i,y_i)$ of validation predictions $\tilde{y}_i(\vec{x}(t))$ of the GBM-DT$^{\ast}$ model over all 25 different targets}
    \label{fig: cum_corr_val_preds_all_targets}
\end{figure}
\\
Because one wants to reduce overall risk of the model to specific feature exposures one could add additional targets to the ensemble model which do not have ultimately the best performance but rather the least correlation to the main target. By doing this one could diversify the feature exposures of the ensemble-model. This would result in a trade-off between the performance of the model and once again ``feature exposure''. This will be neglected and only the best performing targets will be used for simplification.
%Through comparing all of the correlation values in the correlation matrix of the given targets one can calculate the least correlated target: \textit{target alan}.
To sum up we'll use the following $N = 5$ targets, which describe 20-day stock market returns, to create the ensemble model: \\
\begin{itemize}
    \item \textit{target cyrus} (main target)
    \item \textit{target nomi}
    \item \textit{target victor}
    \item \textit{target ralph}
    \item \textit{target bravo}
\end{itemize}
The weights $\omega_i(\vec{x})$ of the model combination will be set equally for simplification. One can optimize this by searching the right weights also by Bayesian Optimization. \\
Additionally the predictions have to be ranked by percentile ($PR[\cdot]$) for normalization ($\tilde{y} \in$ [0,1]) to be compatible for submission.
\begin{center}
    $\tilde{y} = PR[F_E(\vec{x})] = PR[\sum_{i=1}^{N=5} \omega_i(\vec{x}) F_i] \quad$ with $\omega_i(\vec{x}) = \omega = \frac{1}{N=5}$
\end{center}
Whereby the percentile rank is defined as follows:
\begin{equation}
    PR = \frac{CF \pm 0.5 \times F}{N} \times 100
\end{equation}
Where $CF$ denotes the cumulative frequency (count of all scores less / more than or equal to the score of interest), $F$ the frequency for the score of interest and $N$ the number of scores in the distribution. It describes of a given score ($\tilde{y}$) the percentage of scores in its frequency distribution that are less than that score. This ranking metric is used in all predictions $\tilde{y}$ which get submissioned to \textit{Numer.ai}. \\
\\
\textbf{Cumulative Correlation:} \\
This very specific (performance) metric evaluation can be used due to the time-sequential behavior of the predicted problem. The trained model will be used to predict on the validation dataframe. Those predictions $\tilde{y}_i$ which in a sense are time-dependent ($\tilde{y}_i(t)$) are used to calculate the correlation on each time step (era) $t_i$ with the ``true'' prediction $y_i$ from the validation dataset and summing them up: $\Sigma_i \text{corr}(\tilde{y}_i,y_i)$. So consequently the ``ideal regressor'' would predict on each time step the true target value and therefor would result in the length of the validation dataset (total rows $N_{val}$).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Neutralization of feature exposure}
\label{sec: neutralizing}
When predicting stochastically behaving systems like the stock market it's very tough to treat this ``non-stationary" relationship between features and returns. Features that have strong predictive power during some periods may not have any in other time periods or may even completely reverse. This specific uncertainty is what is called ``feature risk" which get induced through the specific ``exposure" as already mentioned. Consequently \textit{feature exposure} stands for the risk exposure of certain features. \\
We can evaluate from the dataset that they're single features which have a high correlation with the target over time (eras). When calculating the individual features exposures, which is a measurement of the Pearson correlation between a model's predictions and each feature, one can obtain that the model is indeed highly correlated to some specific features [Fig. \ref{fig: per_era_correlation}], whereby the features groups which where structured by \textit{Numer.ai} (in total 8 feature groups) were used.
\begin{figure}[!htpb]%[htp]
    \centering
    \includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{figures/per_era_correlations.png}
    \caption[Cumulative per era correlation of the different feature groups]{Cumulative per era correlation $\Sigma_i \text{corr}(\tilde{y}_i,y_i)$ of the different feature groups validation predictions $\tilde{y}_i(\vec{x}(t))$ of the GBM-DT$^{\ast}$ model over the main \textit{target cyrus} $y(\vec{x}(t))$}
    \label{fig: per_era_correlation}
\end{figure}
\\
It has to be said that the Pearson correlation coefficient measures linear correlation between two sets of data and therefor higher order relations are not taken into account at this point. The idea is to use ``Feature Neutralization" to reduce feature exposure in a sense of eliminating stationary relationships best possible, as \textit{Numer.ai} proposed. This ``neutralizaton'' process, at a high level, is nothing more than removing the component of the predictions that are \textit{linearly} correlated with certain features, leaving only the residual non-linear component \cite{Numerai}. When reducing only one single feature $x_1$ for simplicity the following consideration has to take place (vectorial notation is neglected at this point). Without loss of generality, one can assume that the real target value $y$ is determined by the following function:
\begin{equation}
\label{eq: neutral_x1}
    y = f(x_1) + g(x_1, x_2,...)
\end{equation}
Whereby the component $f(x_1)$ only contributes $x_1$ to $y$. Under the assumption of ignoring terms above the second order (only linear dependency is considered), the neutralization for $x_1$ is equivalent as deleting $f(x_1)$. This result can be obtained through a calculation to find $\alpha$ and $\beta$ that minimize the squared error of equation (\ref{eq: neutral_x1}).
\begin{equation}
    \label{eq: neutral_scalar}
    f(x_1) = \alpha x_1 + \beta \xrightarrow{f(x_1) \overset{!}{=} 0} y' = g(x_1,x_2,...)
\end{equation}
Which results consequently in $\beta = - \alpha x_1$, where one has to find the $\beta$ values which reduces the linear relationship. Because only first-order contributions to $y$ will be vanished, this argument does not hold if the absolute value of the feature value is large \cite{Bobyfish2022}. But because the dataset is normalized this is not the case and consequently it is a solid approximation to remove linear correlation (first order) between a feature $x_i$ and the target $y$ (``feature risk reduction").\\
In order to implement this process for all features this structure can be similarly written as:
\begin{equation}
    \vec{\beta} \overset{!}{=} \hat{A}' \cdot \vec{y}
\end{equation}
where $\hat{A}$ denotes the theoretical (dataset)-matrix which includes all features values $x_i$ over time (eras) [Ref. Fig. \ref{fig: numerai_dataset}] and $\vec{y}$ is the target vector (real target values not predictions). But because $\hat{A}^{mn}$ has more rows than columns ($m > n$) the matrix $\hat{A}$ is not invertible per se, and therefore \textit{Numer.ai} \cite{Bobyfish2022} proposes using the \textit{Moore Penrose matrix} for finding the matrix's inverse. \\
\begin{center}
\begin{bmatrix}
a_{11} & a_{12} & .. & .. \\
a_{21} & a_{22} & .. & .. \\
.. & .. & .. & .. \\
.. & .. & .. & a_{mn} \\
\end{bmatrix}
\begin{bmatrix}
\beta_1 \\
.. \\
.. \\
\beta_n \\
\end{bmatrix}
=
\begin{bmatrix}
y_1 \\
.. \\
.. \\
y_m \\
\end{bmatrix}
\[\text{where} \quad m > n \quad \text{or} \quad \hat{A}' \vec{\beta} = \vec{y} \]
\end{center}
This ``pseudo-inverse" matrix $\hat{A}_{MP}$ minimizes the mentioned least-squared error:
\begin{equation}
    \underbrace{ \hat{A} \hat{A}_{MP} }_{\approx \hat{I}} \vec{\beta} \approx \hat{A}_{MP} \vec{y} \rightarrow \vec{\beta} \approx \hat{A}_{MP} \cdot \vec{y}
\end{equation}
So for the feature neutralization process one has to take the whole dataframe, where the columns have to be neutralized as input. This produces a solution with the least squared error through the $\vec{\beta}$-vector where each $\beta_i$- represents the coefficients of a least squares linear solution \cite{Bobyfish2022}. The neutralized predictions will result in the following form:
\begin{equation}
    \vec{y}_{neutral} = \vec{y}_0 - \epsilon \underbrace{\hat{A} \cdot \overbrace{\left( \hat{A}_{MP} \cdot \vec{y}\right)}^{\approx \vec{\beta}} }_{\vec{y}_{\beta}}
\end{equation}
Here $\epsilon$ is just a proportion constant and $\vec{y}_0$ are the initial predictions.
This prediction is then multiplied by the desired proportion and subtracted from the original score vector to create a new one and the feature exposure reduction schematic (``neutralization") is completed. When setting $\epsilon = 1$ one get the following expression similary to equation (\ref{eq: neutral_scalar}):
\begin{center}
    $\vec{y}_{neutral} = \vec{y}_0 - \vec{y}_{\beta}$ of the form $y' = y - f(x_1)$
\end{center}
The whole neutralization process results in a trade-off between an increase in consistency in predictions and feature / target correlation [Fig. \ref{fig: feature_neutral}]. Consistency stands here for continuous positive signals (trades) over time which is equivalent to holding risk low of the model (reducing ``feature exposure").
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.6\textwidth,trim={0 50 0 0},clip]{figures/feature_exposure.png}
    \caption[Feature neutralization trade-off]{Feature neutralization trade-off between consistency and correlation over feature exposure with reference to \cite{Bobyfish2022}}
    \label{fig: feature_neutral}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Scoring methods}
For evaluating the scores of our models one can use different metrics. The standard process would be to use the correlation from the calculated predictions $\hat{y}$ with the provided target values $y$. But because we're dealing with a dataframe from the financial markets we can introduce two additional performance metrics like ``sharpe''-ratio and ``maximal drawdown''. The sharpe ratio measures the performance of an investment $R_a$ compared to a risk-free asset $R_b$ (expected value of the excess of the asset return over the benchmark return), after adjusting for its risk.
\begin{equation}
    \label{eq: sharpe_ratio}
    S_a = \frac{\mathbb{E}[R_a - R_b]}{\sigma_a}
\end{equation}
The maximum drawdown is the maximum observed loss from a peak over the history of the variable, before a new peak is attained. So it indicates downside risk over a specified time period.
\begin{equation}
\label{eq: mdd}
    MDD = \underset{\tau \in (0,T)}{\max} D(\tau) =  \underset{\tau \in (0,T)}{\max} [ \underset{t \in (0,\tau)}{\max} X(t) - X(\tau)]
\end{equation}
In addition there's a predefined correlation function $C_{nmr}$ \cite{Numeraicorr}, which is the primary scoring metric. It uses a special variation of correlation. At a high level, this metric is designed to be a good approximation for actual portfolio returns if the predictions were used in live trading according to \textit{Numer.ai} \cite{Numerai}. \\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Results and Model Interpretation}
Summing up again: To predict the future stock price returns of this data science problem the fundamental builds the Gradient Boosting regression model based on Decision Trees. Through Bayesian Optimization the best hyperparameter combination via the CV metric will be evaluated to get the best possible output. The model (shorthanded as GBM-DT$^{\ast}$) will be trained on the \textit{training dataframe} via different targets to create the ensemble model to mitigate risk best possible. Through evaluating the trained model on the \textit{validation dataframe} via the scoring metrics the overall performance will be measured. Finally the \textit{live predictions} can be generated by using the trained ensemble model to predict on the \textit{live dataset} and submitting the predictions to \textit{Numer.ai} to receive a stake on the meta-model. The building blocks of this supervised machine learning methods are listed once again:
\begin{itemize}
    \item Gradient Boosting Machine based on Decision Trees
    \item 5-fold Cross Validation metric
    \item Bayesian Optimizer
    \item Selection metric for ensemble model
    \item Score metrics for evaluation
\end{itemize}
Those components are very similar in different supervised machine learning methods regardless of whether one deals with a regression or classification prediction task. Even when working with Neural Networks (NN) one will use a similar data-processing work flow for creating a prediction model or perform inference.
\subsection{Comparison with a Neural Network model}
Because of this similar process the performance of the final GBM-DT$^{\ast}$ model will be compared shortly to a ``Deep'' Neural Network. Because of simplicity a pre-built framework namely \textit{PyTorch} \cite{PytorchNN} will be used for creating the Neural Network. The NN will be designed as classical pyramid-structured MLP (Multilayer Perceptron) Neural Network [\ref{fig: nn_mlp}].
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{figures/The-structure-of-a-multilayer-perceptron-neural-network.png}
    \caption[Multilayer Perceptron Neural Network design structure]{Multilayer Perceptron Neural Network design structure with the input layer, hidden layers and output layer / $\omega_{ij}$ denotes the weights and $\sigma$ the activation function of each neuron [\cite{Faghfouri2011} Fig. 3]}
    \label{fig: nn_mlp}
\end{figure}
One input feature vector $\vec{\chi}_i$ is implemented as one input-neuron in the input layer. Due to the fact that the predicted output $\tilde{y}$ is a scalar the output layer will only consist of one neuron (regression problem). Between input and output layer 3 hidden layers will be interposed. So the total structure results to: $N_{features} = 583$ input neurons / 1000 neuron at the 1st hidden layer / 500 neurons at the 2nd hidden layer / 250 neurons at the layer 3rd hidden layer / 1 neuron at the output layer \\
As activation function $\sigma$ the classical ``ReLu'' -function will be implemented, which is a standard choice when dealing with tabular data. For optimizing the loss in the training cycle the \textit{Adam-Optimizer} \cite{Pytorch} will be used, which is a adaptive stochastic gradient descent optimization algorithm. For the loss function the mean-squared error (MSE) is used again. Other hyper-parameters are: \\
\begin{itemize}
    \item Learning rate $\nu$ = 0,01
    \item Number of epochs $n_{epochs}$ = 100
    \item Batch size $b_{size}$ = 50
\end{itemize}
Whereby the learning rate $\nu$ shrinks down the calculated change on every weight $\Delta \omega_{ji}$ in the network according to equation [\ref{eq: nn_mlp_weights}] when using steepest (gradient) descent:
\begin{equation}
    \label{eq: nn_mlp_weights}
    \Delta \omega_{ji}(n) = - \nu \frac{\partial \epsilon(n)}{\partial v_j(n)} y_i(n)
\end{equation}
Where $y_i(n)$ is the output of the previous neuron and $\frac{\partial \epsilon(n)}{\partial v_j(n)}$ denotes the partial derivative from the error $\epsilon(n)$ according to the weighted sum $v_j(n)$ of the input connections of the neuron $i$.
With the number of epochs $n_{epochs}$ one defines the iteration rounds / complete passes through the training dataset. The batch size is a parameter that controls the number of training samples to work through before the model's loss / error is calculated and internal parameters are updated accordingly. The batch size $b_{size}$ is a hyper-parameter of gradient descent that controls the number of training samples to work through before the model’s internal parameters are updated. \\
This very simple Neural Network can be used as alternative supervised machine learning algorithm whereby the performance in terms of computational resources and scoring will be analyzed shortly. The hyper-parameters of the neural network will not be optimized and are set based on experience with similarly large data sets. The pseudo-algorithm for training the neural network with the training dataset $\hat{\mathcal{X}}$ is roughly sketched in [\ref{alg: nn_alg_sketch}]:
\begin{algorithm}
\caption{Multilayer Perception Neural Network}
\label{alg: nn_alg_sketch}
    MLP-NN initialization (layer composition, activation function, loss function)\\
    train / test split of training data $\hat{\mathcal{X}}$ and $\hat{\mathcal{Y}}$ \\
    \For{$epoch$ in $n_{epochs}$}
    {
    \For{$b_{size}$ in $total_{rows}$}
    {
    sub-sample the training data $\hat{\mathcal{X}}$ and $\hat{\mathcal{Y}}$ through batch size $b_{size}$ \\
    calculate predictions $\hat{y}$ by the MLP-NN \\
    calculate loss / error $\epsilon(n)$ of the batched $\hat{\mathcal{X}}$ and $\hat{\mathcal{Y}}$\\
    optimize the network through gradient descent by backward propagation \\
    }
    calculate the error $\epsilon(n)$ of the predictions $\hat{y}$ and test values
    }
\end{algorithm}
\\
The written model is once again visible in the Github repository \cite{Gschaider}. In the following the MLP-NN$^{\ast}$ model (non-optimized) will be compared with the GBM-DT$^{\ast}$ model (optimized hyper-parameters) and both got trained on the same \textit{target cyrus}. The training runtime is significantly lower for the GBM-DT$^{\ast}$ model which takes approximately 17 min compared to the MLP-NN$^{\ast}$ model with 20 hours, whereby it has to be said that this depends mainly on the chosen batch size $b_{size}$ and training epochs $n_{epochs}$. Here only 10 epochs are selected with a batch size of 500 training samples.
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.6\textwidth,trim={0 0 0 0},clip]{figures/2023-01-19_loss_deviance_mse_nn_n_epochs=10_batch_size=500.png}
    \caption[Loss deviance from the Neural Network training]{Loss deviance $\Delta$/ MSE of the Neural Network MLP-NN$^{\ast}$ model training over the main \textit{target cyrus} over 10 $n_{epochs}$ witch batch size $b_{size}$ = 500}
    \label{fig: nn_loss}
\end{figure}
The loss reduction is visible in figure [\ref{fig: nn_loss}] and the performance of the validation predictions for both models is depicted in figure [\ref{fig: cum_corr_val_preds_gbm_nn}].
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{figures/2023-01-19_cumulative_correlation_of_validation_predicitions_gbm_nn_n_epochs=10_batch_size=500.png}
    \caption[Cumulative per era correlation of validation predictions over the GBM-DT$^{\ast}$ models with different targets]{Cumulative per era correlation $\Sigma_i \text{corr}(\tilde{y}_i,y_i)$ validation predictions $\tilde{y}(\vec{x}(t))$ of the GBM-DT$^{\ast}$ (optimized) and MLP-NN$^{\ast}$ (non-optimized) model over the main \textit{target cyrus}}
    \label{fig: cum_corr_val_preds_gbm_nn}
\end{figure}
It is clearly visible that the predictions of both models follow a similar path. Nevertheless the MLP-NN$^{\ast}$ results in a weaker performance which underlines the strength of the GBM-DT framework. But it has to be said that the optimization of the hyper-parameters of the MLP-NN model will be make a huge impact, as it was with the GBM-DT model.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{GBM-DT model performance}
Now we'll switch to the GBM-DT$^{\ast}$ model and train it not only on the main \textit{target cyrus} but also on other targets and analyze the individual performances. We saw the performance of all targets in figure [\ref{fig: cum_corr_val_preds_all_targets}] and based on that 5 targets are selected because of best performance. In the following figure [\ref{fig: cum_corr_val_preds_ensemble}] the cumulative correlation of those validation predictions (inference) are shown.
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{rounds/2024-01-17_round1_cumulative_correlation_of_validation_predicitions_ensemble.png}
    \caption[Cumulative per era correlation of validation predictions over the GBM-DT$^{\ast}$ ensemble model with comparison]{Cumulative per era correlation $\Sigma_i \text{corr}(\tilde{y}_i,y_i)$ of validation predictions $\tilde{y}(\vec{x}(t))$ of the GBM-DT$^{\ast}$ over best performing targets incl. ensemble model}
    \label{fig: cum_corr_val_preds_ensemble}
\end{figure}
Additionally the evaluations of the core metrics are listed in the following summary table [\ref{table: summary_metric_predictions_ensemble}]. This includes the mean of the predicted target values mean[$\tilde{y}_i(\vec{x})$], the standard deviation of the predicted targets $\sigma_i(\vec{x})$, the sharpe ratio $S_i$ (equ. \ref{eq: sharpe_ratio}) whereby the risk free yield is set to 0 \%, the maximum drawdown $MDD$ (equ. \ref{eq: mdd}) and the mean correlation $C_{iCyrus}$ with \textit{target cyrus}, as it as the main target for prediction.
\begin{comment}
\begin{table}[!htbp]
\centering
\caption{Summary metrics of prediction values from different targets \\
predicted target values $\tilde{y}_i(\vec{x})$ \\
standard deviation of the predicted targets $\sigma_i(\vec{x})$ \\
sharpe ratio $S_i$ (\ref{eq: sharpe_ratio}) with risk free yield 0 \% \\
maximum drawdown $MDD$ (\ref{eq: mdd}) \\
mean correlation $C_{iCyrus}$ with \textit{target cyrus} \\}
\label{table: summary_metric_predictions}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
\textit{targets} & mean[$\tilde{y}_i(\vec{x})$] & $\sigma_i(\vec{x})$ & $S_i$ & $MDD$ & $C_{iCyrus}$ \\ \hline
\hline
\hline
\textit{cyrus} & 0,0191 & 0,0207 & 0,9214 & 0,0510 & 1,0  \\ \hline
\textit{alan} & 0,0107 & 0,0213 & 0,4995 & 0,1000 & 0,6650 \\ \hline
\textit{nomi} & 0,0191 & 0,0207 & 0,9217 & 0,0393 & 0,8248 \\ \hline
\textit{victor} & 0,0184 & 0,0187 & 0,9852 & 0,0456 &  0,8290 \\ \hline
\end{tabular}
\end{table}
\end{comment}
\begin{table}[!htbp]
\centering
\caption{Summary metrics of prediction values from the ensemble model and different targets \\
predicted target values $\tilde{y}_i(\vec{x})$ \\
standard deviation of the predicted targets $\sigma_i(\vec{x})$ \\
sharpe ratio $S_i$ (\ref{eq: sharpe_ratio}) with risk free yield 0 \% \\
maximum drawdown $MDD$ (\ref{eq: mdd}) \\
mean correlation $C_{iCyrus}$ with \textit{target cyrus} \\}
\label{table: summary_metric_predictions_ensemble}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
\textit{targets} & mean[$\tilde{y}_i(\vec{x})$] / 1 & $\sigma_i(\vec{x})$ / 1 & $S_i$ / 1 & $MDD$ / \% & $C_{iCyrus}$ / 1 \\ \hline
\hline
\hline
\textit{cyrus} & 0,0137 & 0,0206 & 0,6656 & -7,00 & 1,0  \\ \hline
\textit{nomi} & 0,0129 & 0,0212 & 0,6052 & -8,82 & 0,8124 \\ \hline
\textit{victor} & 0,0119 & 0,0187 & 0,6360 & -7,85 &  0,7990 \\ \hline
\textit{ralph} & 0,0128 & 0,0202 & 0,6331 & -6,32 & 0,8031 \\ \hline
\textit{bravo} & 0,0136 & 0,0197 & 0,6904 & -5,70 & 0,9650 \\ \hline
\hline
\textit{ensemble} & 0,0139 & 0,0208 & 0,6664 & -7,48 & 0,9337 \\ \hline
\end{tabular}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
We can obtain that beside the main \textit{target cyrus} the \textit{target bravo} (very high correlation with \textit{cyrus}) and \textit{victor} result in nearly identical sharpe ratios (a high sharpe ratio is good) over the validation dataframe whereby \textit{target nomi} results in the lowest sharpe ratio in comparison. The maximum drawdown is lowest on \textit{bravo} but highest on \textit{nomi}, which has to be held low in the overall model.
When including targets which perform worse in the long run this ``risk'' factor will increase, which should be avoided.
The main \textit{target cyrus} performs consistently the best, whereby \textit{target victor} performs worst. \\
Further we'll take a look on the feature importances (as stated in section [\ref{sec: decision_trees}]) of the (sub)-models [Fig. \ref{fig: fi_cyrus} - \ref{fig: fi_bravo}].
\begin{figure}[htbp]
\begin{minipage}[t]{8cm}
\vspace{0pt}
\centering
\includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{rounds/2024-01-17_round1_feature_importance_target_cyrus_v4_20.png}
\caption[Feature importance of GBDT model over target cyrus]{Feature importance of GBM-DT$^{\ast}$ model over target \textit{cyrus}}
\label{fig: fi_cyrus}
\end{minipage}
\hfill
\begin{minipage}[t]{8cm}
\vspace{0pt}
\centering
\includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{rounds/2024-01-17_round1_feature_importance_target_nomi_v4_20.png}
\caption[Feature importance of model nomi]{Feature importance of GBM-DT$^{\ast}$ model over target \textit{nomi}}
\label{fig: fi_nomi}
\end{minipage}
\end{figure}
We can obtain that on all models except \textit{bravo} the most important feature in the weighting process is \textit{wetter unbaffled loma}.
At model \textit{bravo} this feature is ranked on place 2, whereby feature \textit{sophical agitato theatricality} is here top ranked. Interestingly this feature is ranked at place 2 to 4 at the other models. Models which rely on different important features do not tend to collectively overestimate single features in their prediction ability in general. Therefor the feature exposure of the ensemble model is held low best possible.
\begin{figure}[htbp]
\begin{minipage}[t]{9cm}
\vspace{0pt}
\centering
\includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{rounds/2024-01-17_round1_feature_importance_target_victor_v4_20.png}
\caption[Feature importance of model victor]{Feature importance of GBM-DT$^{\ast}$ model over target \textit{victor}}
\label{fig: fi_victor}
\end{minipage}
\hfill
\begin{minipage}[t]{9cm}
\vspace{0pt}
\centering
\includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{rounds/2024-01-17_round1_feature_importance_target_ralph_v4_20.png}
\caption[Feature importance of model ralph]{Feature importance of GBM-DT$^{\ast}$ model over target \textit{ralph}}
\label{fig: fi_ralph}
\end{minipage}
\end{figure}

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.5\textwidth,trim={0 0 0 0},clip]{rounds/2024-01-17_round1_feature_importance_target_bravo_v4_20.png}
    \caption[Feature importance of model bravo]{Feature importance of GBM-DT$^{\ast}$ model over target \textit{bravo}}
    \label{fig: fi_bravo}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The individual trained GBM-DT$^{\ast}$ models are used to create the ensemble model as it was already visible in figure [\ref{fig: cum_corr_val_preds_ensemble}]. As stated the model weights $\omega_i$ will be set all equally to $\frac{1}{N=5}$ and the resulting prediction values $\tilde{y}$ will be ranked by percentile because of normalization. As it was already said, the goal of this model combination (ensemble modeling) is to reduce the risk of a single model and to diversify predictions based on different features. \\
As stated in section [\ref{sec: neutralizing}] \textit{feature neutralization} proposed by \textit{Numer.ai} can be another very important step in that type of stochastical prediction problem towards a better behaving model over the long run. \textit{Numer.ai} proposes here an elegant way to reduce the feature exposure by neutralizing a whole feature group [Fig. \ref{fig: cum_corr_val_preds_ensemble_neutral}]. Because of this untypical metric at machine learning tasks we'll take a closer look towards the resulting effects. \\
\begin{figure}[!htpb]
    \centering
    \includegraphics[width=1\textwidth,trim={0 0 0 0},clip]{rounds/2024-01-17_round1_cumulative_correlation_of_validation_predicitions_neutralization_ensemble.png}
    \caption[Cumulative per era correlation of neutralized predictions of the GBM-DT$^{\ast}$ ensemble model with comparison]{Cumulative per era correlation $\Sigma_i \text{corr}(\tilde{y}_i,y_i)$ of neutralized validation predictions $\tilde{y}(\vec{x}(t))$ of the GBM-DT$^{\ast}$ ensemble model with comparison}
    \label{fig: cum_corr_val_preds_ensemble_neutral}
\end{figure}
We can see that by \textit{neutralizing} the feature group \textit{serenity} one gets the best cumulative per era correlation $\Sigma_i \text{corr}(\tilde{y}_i,y_i)$ within the targets. It has to be noted once that the neutralization process is carried out on the ensemble model, so on multiple targets (in total $N = 5$). \\
Projecting this one the per era correlation of the whole \textit{serenity} group which was plotted in figure [\ref{fig: per_era_correlation}] one can definitely obtain features with higher fluctuations (a higher variance) and less per era correlation and that is indeed interesting: by \textit{neutralizing} the feature group which had the least per era correlation with the target results in the best overall performance of the model. So by removing the linear part of the feature group with the least linear correlation one gets the best performance and in addition the specific ``feature exposure'' of this group will be reduced. Consequently consistency in the predictions by the model will be increased and therefor the scoring ratios as shown in table [\ref{table: summary_metric_predictions_neutral_ensemble}]. \\
It is clearly evident that the Sharpe ratio results in a higher value of $S_{n.Ens.} = 0{,}7002$ over the given validation time-period (eras). The volatilities $\sigma_{i}(\vec{x})$ of the models are nearly identical. The maximum drawdown $MDD$ is higher for the neutralized model which ultimately results in a higher correction. \\
The correlation with \textit{target cyrus} over the validation period is indeed lower but because of the higher performance of the model. \\
Another possible optimization of the model would be the neutralization of multiple feature groups instead of a single one. One would argue that if the neutralization of two seperated groups give better predictions results than the ensemble model solely, neutralizing multiple groups can only lead to better performance. But one has to keep in mind that due the fact that the dataset gets updated weekly (non-stationary) this might change over time. Therefor the continuously review of the model performance is mandatory for getting consistent results.
Finally one can achieve descent performance results with this built and trained model on this financial prediction problem which was provided by \textit{Numer.ai} \cite{Numerai}.
\begin{table}[!htbp]
\centering
\caption{Summary Metrics of prediction values from the ensemble and neutralized ensemble model \\
predicted target values $\tilde{y}_i(\vec{x})$ \\
standard deviation of the predicted targets $\sigma_i(\vec{x})$ \\
sharpe ratio $S_i$ (\ref{eq: sharpe_ratio}) with risk free yield 0 \% \\
maximum drawdown $MDD$ (\ref{eq: mdd}) \\
mean correlation $C_{iCyrus}$ with \textit{target cyrus} \\}
\label{table: summary_metric_predictions_neutral_ensemble}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
\textit{targets} & mean[$\tilde{y}_i(\vec{x})$] / 1 & $\sigma_i(\vec{x})$ / 1 & $S_i$ / 1 & $MDD$ / \% & $C_{iCyrus}$ / 1 \\ \hline
\hline
\hline
\textit{ensemble} & 0,0139 & 0,0208 & 0,6664 & -7,48 & 0,9337 \\ \hline
\hline
\textit{neutral. ensemble} & 0,0145 & 0,0208 & 0,7002 & -9,60 & 0,9254 \\ \hline
\end{tabular}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Conclusion}
Gradient Boosting is a powerful tool in the area of supervised machine learning. The implementation with Decision Trees as base learners provides the possibility to create a prediction model where non-linearity plays a significant role. \\
Because of the design of the framework one can easily analyze the decision branching which are made to split the data by the algorithm. This ``transparency'' of the system can be a huge benefit compared to other techniques like Neural Networks, which are indeed kind of ''black-box'' systems. Additionally, the performance was better at a lower training time. Although the hyperparameters of the MLP-NN model have not been optimized, it underlines the strengths of the Gradient Boosting Machine framework. For a solid comparison one would need to analyze the MLP-NN framework more in depth. But when dealing with tabular data like it is the case with the stated real-world data problem a boosting framework is always a first good choice \cite{McElfresh2023}. \\
Bayesian Optimization acts from an abstract point of view as the inner clock working for tuning the model with the right hyper-parameters, which is key for finding the best possible model as shown. For prediction tasks where multiple targets are given one has to evaluate clearly which one to choose. Here is the possibility to use further informative algorithms like Bayesian Model Averaging (BMA) to dynamically adjust the current sub-models by assigning them optimal weights for the given set. Furthermore there's always the possibility to use other supervised machine learning algorithms like Recurrent Neural Networks (RNN), Support Vector Machines (SVM) or Random Forests (RF).
Here, of course, it is also necessary to optimize the parameters or tune the layout in general.\\
When dealing specifically with time-series datasets like the one from \textit{Numer.ai} it can be for sure a good alternative to go with a statistical time-series analysis tool (e.g. ARMA). It must be said that normally a GBM-DT framework will not be the first choice solution when predicting a time-series problem. Nevertheless it can be a powerful and even useful tool when the data is aligned in a tabular form. \\
In general one has to keep in mind when preparing the data on its own for a machine learning task that normalization or standardization plays a significant role for preventing overfitting. Here, in addition the Bias-Variance trade-off has to be considered very inherently. The trade-off between the complexity of a model and the capability of generalization to a new dataset is key when dealing with supervised machine learning tasks. A too simple model can have a high bias and can not represent the complexity of the data well. Consequently a complex model can have low bias but a high variance, what would result also in overfitting and in a poor generaliszation of the data. But when using a ensemble machine learning algorithm like GBM-DT with the right cross-validation metric and an informative hyperparameter optimization technique one gets a huge step closer to a robust and predictive model. \\
Finally, it must be said that due to the constant updating of the data (non-stationary data inference problem) of \textit{Numer.ai} \cite{Numerai}, the evaluations will change over time.
\section{Final thoughts and acknowledgment}
First, I would like to thank my supervisor Prof. Dr. Wolfgang Von der Linden and Dr. Sascha Ranftl for giving me this opportunity to work on a present and still growing topic in the field of machine learning. Even if it's not specifically a pure physics topic the application of modern prediction algorithms can be mandatory when dealing with a huge amount of data. \\
But ultimately, and that's perhaps the most important one, I was able to
introduce and expand my knowledge, which I gathered in the past three years of studying physics, in this bachelor thesis. Last but not least, I thank my parents Hannes and Daniela for enabling me the study of Physics in Graz.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\printbibliography
\newpage
\listoffigures
\end{document}
